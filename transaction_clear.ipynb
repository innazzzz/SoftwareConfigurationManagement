{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZCXrIKSjVAe",
        "outputId": "dc727d98-722e-4326-ab2a-ebb64f4368d5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clearml\n",
            "  Downloading clearml-2.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.12/dist-packages (from clearml) (25.4.0)\n",
            "Collecting furl>=2.0.0 (from clearml)\n",
            "  Downloading furl-2.1.4-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from clearml) (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.12/dist-packages (from clearml) (2.0.2)\n",
            "Collecting pathlib2>=2.3.0 (from clearml)\n",
            "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.12/dist-packages (from clearml) (5.9.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from clearml) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from clearml) (2.9.0.post0)\n",
            "Requirement already satisfied: pyjwt<2.11.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from clearml) (2.10.1)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.12/dist-packages (from clearml) (6.0.3)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from clearml) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from clearml) (2.5.0)\n",
            "Requirement already satisfied: Pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from clearml) (11.3.0)\n",
            "Requirement already satisfied: referencing<0.40 in /usr/local/lib/python3.12/dist-packages (from clearml) (0.37.0)\n",
            "Requirement already satisfied: requests>=2.32.0 in /usr/local/lib/python3.12/dist-packages (from clearml) (2.32.4)\n",
            "Collecting orderedmultidict>=1.0.1 (from furl>=2.0.0->clearml)\n",
            "  Downloading orderedmultidict-1.0.2-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6.0->clearml) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6.0->clearml) (0.30.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing<0.40->clearml) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->clearml) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->clearml) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.0->clearml) (2025.11.12)\n",
            "Downloading clearml-2.1.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading furl-2.1.4-py2.py3-none-any.whl (27 kB)\n",
            "Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
            "Downloading orderedmultidict-1.0.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pathlib2, orderedmultidict, furl, clearml\n",
            "Successfully installed clearml-2.1.0 furl-2.1.4 orderedmultidict-1.0.2 pathlib2-2.3.7.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install clearml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clearml-agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpKdMWkzd700",
        "outputId": "169a5c53-17db-48a4-cfa0-6cdc7793ffc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clearml-agent\n",
            "  Downloading clearml_agent-2.0.6-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: psutil<7.1,>=3.4.2 in /usr/local/lib/python3.12/dist-packages (from clearml-agent) (5.9.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from clearml-agent) (2.5.0)\n",
            "Collecting virtualenv<21,>=16 (from clearml-agent)\n",
            "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: requests<2.33,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from clearml-agent) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from clearml-agent) (75.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<2.33,>=2.20.0->clearml-agent) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<2.33,>=2.20.0->clearml-agent) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<2.33,>=2.20.0->clearml-agent) (2025.11.12)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<21,>=16->clearml-agent)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.12/dist-packages (from virtualenv<21,>=16->clearml-agent) (3.20.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv<21,>=16->clearml-agent) (4.5.1)\n",
            "Downloading clearml_agent-2.0.6-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv, clearml-agent\n",
            "Successfully installed clearml-agent-2.0.6 distlib-0.4.0 virtualenv-20.35.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (–≤–∞—à–∏ –∫–ª—é—á–∏ —Å —Å–∞–π—Ç–∞ app.clear.ml)\n",
        "os.environ[\"CLEARML_API_ACCESS_KEY\"] = \"0KXRS8L8RMU542OO7V1GY6VBWCEOJ5\"\n",
        "os.environ[\"CLEARML_API_SECRET_KEY\"] = \"Q3SghOk7iksdcic_04kkc-JJlS5f-JOXMOPxAI-cOBjK7Ar8JZmfdVpuSr3Wzgi356E\"\n",
        "os.environ[\"CLEARML_WEB_HOST\"] = \"https://app.clear.ml\"  # –ê–¥—Ä–µ—Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞\n",
        "os.environ[\"CLEARML_API_HOST\"] = \"https://api.clear.ml\"  # –ê–¥—Ä–µ—Å API\n"
      ],
      "metadata": {
        "id": "FuS5685fovMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def aggregate_client_daily_items(df)\n",
        "\n",
        "–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ - –≤–∏–∑–∏—Ç—ã"
      ],
      "metadata": {
        "id": "DmGTgtyQVRVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "\n",
        "def aggregate_client_daily_items(df):\n",
        "    \"\"\"\n",
        "    –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–∫—É–ø–∫–∞—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –¥–Ω—è–º –∏ —Ç–æ–≤–∞—Ä–∞–º.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:\n",
        "            clientID, visit_date, item, itemGroup, quantity, amount\n",
        "    \"\"\"\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "    required_columns = ['clientID', 'trDte', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"–í DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}\")\n",
        "\n",
        "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    aggregated_df = df.groupby(\n",
        "        ['clientID', 'trDte', 'item', 'itemGroup'],\n",
        "        as_index=False\n",
        "    ).agg({\n",
        "        'quantity': 'sum',\n",
        "        'amount': 'sum'\n",
        "    })\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n",
        "    aggregated_df = aggregated_df.rename(columns={'trDte': 'visit_date'})\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    aggregated_df = aggregated_df.sort_values(\n",
        "        by=['clientID', 'visit_date', 'item']\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫\n",
        "    result_columns = ['clientID', 'visit_date', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    aggregated_df = aggregated_df[result_columns]\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏:\n",
        "if __name__ == \"__main__\":\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞\n",
        "    df = pd.read_csv('transaction.csv')\n",
        "\n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
        "    result_df = aggregate_client_daily_items(df)\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "    print(\"–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\")\n",
        "    print(result_df.head(10))\n",
        "    print(f\"\\n–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame: {df.shape}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame: {result_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-EgFbqgZDvCo",
        "outputId": "267eda0f-3c58-4f68-e31c-15151ce8eddc",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'transaction.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2845807283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transaction.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transaction.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ClearML** Task prepare\n",
        "\n",
        "def aggregate_client_daily_items(df)"
      ],
      "metadata": {
        "id": "zsSRqdtHVoZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=ZKZG05E8JU92A4BSBKTBWUTCPP9QS4\n",
        "%env CLEARML_API_SECRET_KEY=BC4rC7hc44LaWdZwJIyu9iOfIKDgRhBC3P0lRzf8JONfVyoL5G6UTxvm_gr6Ee36Hyo\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º ClearML\n",
        "from clearml import Task\n",
        "\n",
        "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç\n",
        "from clearml import Task\n",
        "task = Task.init(project_name=\"CourseInz\", task_name=\"prepare\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def aggregate_client_daily_items(df):\n",
        "    \"\"\"\n",
        "    –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–∫—É–ø–∫–∞—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –¥–Ω—è–º –∏ —Ç–æ–≤–∞—Ä–∞–º.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:\n",
        "            clientID, visit_date, item, itemGroup, quantity, amount\n",
        "    \"\"\"\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "    required_columns = ['clientID', 'trDte', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"–í DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}\")\n",
        "\n",
        "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    aggregated_df = df.groupby(\n",
        "        ['clientID', 'trDte', 'item', 'itemGroup'],\n",
        "        as_index=False\n",
        "    ).agg({\n",
        "        'quantity': 'sum',\n",
        "        'amount': 'sum'\n",
        "    })\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n",
        "    aggregated_df = aggregated_df.rename(columns={'trDte': 'visit_date'})\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    aggregated_df = aggregated_df.sort_values(\n",
        "        by=['clientID', 'visit_date', 'item']\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫\n",
        "    result_columns = ['clientID', 'visit_date', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    aggregated_df = aggregated_df[result_columns]\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏:\n",
        "if __name__ == \"__main__\":\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞\n",
        "    df = pd.read_csv('transaction.csv')\n",
        "\n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é\n",
        "    result_df = aggregate_client_daily_items(df)\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "    print(\"–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\")\n",
        "    print(result_df.head(10))\n",
        "    print(f\"\\n–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame: {df.shape}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame: {result_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZefZwI-E8KR",
        "outputId": "8e186134-5895-43ad-95e9-813d132abd25",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=ZKZG05E8JU92A4BSBKTBWUTCPP9QS4\n",
            "env: CLEARML_API_SECRET_KEY=BC4rC7hc44LaWdZwJIyu9iOfIKDgRhBC3P0lRzf8JONfVyoL5G6UTxvm_gr6Ee36Hyo\n",
            "ClearML Task: created new task id=89d57a43ceb14c1181564d380743fd4f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/89d57a43ceb14c1181564d380743fd4f/output/log\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\n",
            "    clientID  visit_date      item                     itemGroup  quantity  \\\n",
            "0    client1  22.01.2018  sku10765                 –õ–∞–∫–∏ –∏ –∫—Ä–∞—Å–∫–∏         1   \n",
            "1    client1  22.01.2018  sku13695                 –°—Ç–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã         5   \n",
            "2    client1  22.01.2018  sku29083                 –õ–∞–∫–∏ –∏ –∫—Ä–∞—Å–∫–∏         2   \n",
            "3    client1  22.01.2018   sku2954                 –õ–∞–∫–∏ –∏ –∫—Ä–∞—Å–∫–∏         1   \n",
            "4   client10  05.08.2019   sku1893                   –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã         1   \n",
            "5   client10  05.08.2019   sku5624                   –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã         1   \n",
            "6   client10  05.08.2019   sku7053                   –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã         1   \n",
            "7  client100  08.05.2019  sku22214  –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∞–¥–∞ –∏ –¥–∞—á–∏         1   \n",
            "8  client100  17.10.2019  sku11580                 –õ–∞–∫–∏ –∏ –∫—Ä–∞—Å–∫–∏         1   \n",
            "9  client100  17.10.2019  sku28198                     –û—Å–≤–µ—â–µ–Ω–∏–µ         1   \n",
            "\n",
            "   amount  \n",
            "0      29  \n",
            "1    1535  \n",
            "2     310  \n",
            "3     399  \n",
            "4      79  \n",
            "5      79  \n",
            "6    4599  \n",
            "7    7299  \n",
            "8      49  \n",
            "9     299  \n",
            "\n",
            "–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame: (1008688, 7)\n",
            "–†–∞–∑–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame: (1003083, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ClearML** Task prepare **–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø**\n",
        "\n",
        "def aggregate_client_daily_items(df)"
      ],
      "metadata": {
        "id": "61-RXP-1V83z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º ClearML\n",
        "from clearml import Task, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"taskprep\",  # –î–æ–±–∞–≤–∏–º –¥–∞—Ç—É –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    tags=[\"data_preprocessing\", \"aggregation\"]\n",
        ")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∑–∞–¥–∞—á–∏\n",
        "task.set_base_docker(\"python:3.9\")  # –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "def aggregate_client_daily_items(df):\n",
        "    \"\"\"\n",
        "    –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–∫—É–ø–∫–∞—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –¥–Ω—è–º –∏ —Ç–æ–≤–∞—Ä–∞–º.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:\n",
        "            clientID, visit_date, item, itemGroup, quantity, amount\n",
        "    \"\"\"\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "    required_columns = ['clientID', 'trDte', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"–í DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}\")\n",
        "\n",
        "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    aggregated_df = df.groupby(\n",
        "        ['clientID', 'trDte', 'item', 'itemGroup'],\n",
        "        as_index=False\n",
        "    ).agg({\n",
        "        'quantity': 'sum',\n",
        "        'amount': 'sum'\n",
        "    })\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¥–∞—Ç—ã –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n",
        "    aggregated_df = aggregated_df.rename(columns={'trDte': 'visit_date'})\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    aggregated_df = aggregated_df.sort_values(\n",
        "        by=['clientID', 'visit_date', 'item']\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫\n",
        "    result_columns = ['clientID', 'visit_date', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    aggregated_df = aggregated_df[result_columns]\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "output_dir = Path(\"preprocessed_data\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "df = pd.read_csv('transaction.csv')\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç\n",
        "print(\"–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞...\")\n",
        "task.upload_artifact(\n",
        "    name=\"raw_data\",\n",
        "    artifact_object='transaction.csv',\n",
        "    metadata={\"rows\": len(df), \"columns\": list(df.columns)}\n",
        ")\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "print(\"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "result_df = aggregate_client_daily_items(df)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ CSV\n",
        "output_file = output_dir / \"transactions_aggregated.csv\"\n",
        "result_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}\")\n",
        "\n",
        "# 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ê–†–¢–ï–§–ê–ö–¢ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)\n",
        "print(\"–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ –∑–∞–¥–∞—á–∏...\")\n",
        "task.upload_artifact(\n",
        "    name=\"aggregated_data\",\n",
        "    artifact_object=str(output_file),\n",
        "    metadata={\n",
        "        \"rows\": len(result_df),\n",
        "        \"columns\": list(result_df.columns),\n",
        "        \"preprocessing_date\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        ")\n",
        "\n",
        "# 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –í–ï–†–°–ò–û–ù–ò–†–û–í–ê–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢ (–±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "dataset = Dataset.create(\n",
        "    dataset_name=\"transactions_aggregated\",\n",
        "    dataset_project=\"CourseInz/Datasets\",  # –ü—Ä–æ–µ–∫—Ç –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
        "    dataset_version=\"1.0.0\",  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Ä—Å–∏—é –∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å None –¥–ª—è –∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞\n",
        "    description=f\"–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤. –°–æ–∑–¥–∞–Ω–æ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\"\n",
        "                f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: transaction.csv\\n\"\n",
        "                f\"–°—Ç—Ä–æ–∫ –¥–æ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {len(df)}, –ø–æ—Å–ª–µ: {len(result_df)}\",\n",
        "    parent_datasets=None  # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        ")\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
        "dataset.add_files(path=str(output_file))\n",
        "\n",
        "# –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫)\n",
        "columns_info = pd.DataFrame({\n",
        "    'column': result_df.columns,\n",
        "    'dtype': result_df.dtypes.astype(str),\n",
        "    'description': [\n",
        "        '–£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞',\n",
        "        '–î–∞—Ç–∞ –ø–æ—Å–µ—â–µ–Ω–∏—è (–≤–∏–∑–∏—Ç–∞)',\n",
        "        '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞',\n",
        "        '–ì—Ä—É–ø–ø–∞ —Ç–æ–≤–∞—Ä–∞',\n",
        "        '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫—É–ø–ª–µ–Ω–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞',\n",
        "        '–°—É–º–º–∞ –ø–æ–∫—É–ø–∫–∏'\n",
        "    ]\n",
        "})\n",
        "columns_info_file = output_dir / \"columns_description.csv\"\n",
        "columns_info.to_csv(columns_info_file, index=False)\n",
        "dataset.add_files(path=str(columns_info_file))\n",
        "\n",
        "# –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\n",
        "metadata = {\n",
        "    \"creation_date\": datetime.datetime.now().isoformat(),\n",
        "    \"source_file\": \"transaction.csv\",\n",
        "    \"original_rows\": len(df),\n",
        "    \"aggregated_rows\": len(result_df),\n",
        "    \"original_columns\": list(df.columns),\n",
        "    \"aggregated_columns\": list(result_df.columns),\n",
        "    \"aggregation_logic\": \"–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ clientID, trDte, item, itemGroup —Å —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º quantity –∏ amount\"\n",
        "}\n",
        "import json\n",
        "metadata_file = output_dir / \"dataset_metadata.json\"\n",
        "with open(metadata_file, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "dataset.add_files(path=str(metadata_file))\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML\n",
        "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä...\")\n",
        "dataset.upload()\n",
        "\n",
        "# –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –Ω–µ–ª—å–∑—è –∏–∑–º–µ–Ω–∏—Ç—å)\n",
        "dataset.finalize()\n",
        "\n",
        "print(f\"–î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {dataset.name}, –≤–µ—Ä—Å–∏—è: {dataset.version}\")\n",
        "print(f\"ID –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset.id}\")\n",
        "\n",
        "# 3. –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤ –∑–∞–¥–∞—á–µ\n",
        "task.set_parameter(\"dataset_id\", dataset.id)\n",
        "task.set_parameter(\"dataset_name\", dataset.name)\n",
        "task.set_parameter(\"dataset_version\", dataset.version)\n",
        "\n",
        "# 4. –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: transaction.csv\")\n",
        "print(f\"–°—Ç—Ä–æ–∫ –¥–æ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {len(df):,}\")\n",
        "print(f\"–°—Ç–æ–ª–±—Ü–æ–≤ –¥–æ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {len(df.columns)}\")\n",
        "print(f\"–°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {len(result_df):,}\")\n",
        "print(f\"–°—Ç–æ–ª–±—Ü–æ–≤ –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {len(result_df.columns)}\")\n",
        "print(f\"–î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {dataset.name} v{dataset.version}\")\n",
        "print(f\"ID –∑–∞–¥–∞—á–∏: {task.id}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI\n",
        "logger = task.get_logger()\n",
        "logger.report_table(\n",
        "    title=\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫)\",\n",
        "    series=\"sample\",\n",
        "    table_plot=result_df.head(10),\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "logger.report_single_value(name=\"original_rows\", value=len(df))\n",
        "logger.report_single_value(name=\"aggregated_rows\", value=len(result_df))\n",
        "logger.report_single_value(name=\"compression_ratio\",\n",
        "                          value=round(len(result_df) / len(df) * 100, 2))\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "print(\"\\n–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\")\n",
        "print(result_df.head(10))\n",
        "print(f\"\\n–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame: {df.shape}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame: {result_df.shape}\")\n",
        "\n",
        "# –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
        "task.close()\n",
        "\n",
        "print(\"\\n‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")\n",
        "print(\"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
        "print(f\"   - –ö–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏: aggregated_data\")\n",
        "print(f\"   - –ö–∞–∫ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {dataset.name} v{dataset.version}\")\n",
        "print(f\"\\nüåê –û—Ç–∫—Ä–æ–π—Ç–µ –≤ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ: https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S61A7XakvZ5u",
        "outputId": "ca087657-2f73-4a07-bdac-7a8071003c65",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
            "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞...\n",
            "–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: preprocessed_data/transactions_aggregated.csv\n",
            "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ –∑–∞–¥–∞—á–∏...\n",
            "–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...\n",
            "ClearML results page: https://app.clear.ml/projects/6f2ef8b98fd943cfb48fed9f231f63ff/experiments/5600efa9440d4c1da5f9b090375ce053/output/log\n",
            "ClearML dataset page: https://app.clear.ml/datasets/simple/6f2ef8b98fd943cfb48fed9f231f63ff/experiments/5600efa9440d4c1da5f9b090375ce053\n",
            "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä...\n",
            "Uploading dataset changes (3 files compressed to 214.41 KiB) to https://files.clear.ml\n",
            "File compression and upload completed: total size 214.41 KiB, 1 chunk(s) stored (average size 214.41 KiB)\n",
            "–î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: transactions_aggregated, –≤–µ—Ä—Å–∏—è: 1.0.0\n",
            "ID –¥–∞—Ç–∞—Å–µ—Ç–∞: 5600efa9440d4c1da5f9b090375ce053\n",
            "\n",
            "==================================================\n",
            "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò:\n",
            "==================================================\n",
            "–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: transaction.csv\n",
            "–°—Ç—Ä–æ–∫ –¥–æ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: 24,908\n",
            "–°—Ç–æ–ª–±—Ü–æ–≤ –¥–æ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: 7\n",
            "–°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: 24,801\n",
            "–°—Ç–æ–ª–±—Ü–æ–≤ –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: 6\n",
            "–î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: transactions_aggregated v1.0.0\n",
            "ID –∑–∞–¥–∞—á–∏: a97b11d13cd848d6a9a7f3d96132dc4b\n",
            "==================================================\n",
            "\n",
            "–ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:\n",
            "      clientID  visit_date      item                     itemGroup  quantity  \\\n",
            "0  client10012  26.10.2017  sku19372                 –°—Ç–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã       1.0   \n",
            "1  client10012  26.10.2017   sku9018                 –°—Ç–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã       2.0   \n",
            "2  client10022  03.09.2017  sku14731    –ö–æ–º–Ω–∞—Ç–Ω—ã–µ —Ä–∞—Å—Ç–µ–Ω–∏—è –∏ —Ü–≤–µ—Ç—ã       1.0   \n",
            "3  client10022  03.09.2017   sku1666    –ö–æ–º–Ω–∞—Ç–Ω—ã–µ —Ä–∞—Å—Ç–µ–Ω–∏—è –∏ —Ü–≤–µ—Ç—ã       1.0   \n",
            "4  client10022  03.09.2017  sku19829    –ö–æ–º–Ω–∞—Ç–Ω—ã–µ —Ä–∞—Å—Ç–µ–Ω–∏—è –∏ —Ü–≤–µ—Ç—ã       1.0   \n",
            "5  client10022  03.09.2017  sku21758    –ö–æ–º–Ω–∞—Ç–Ω—ã–µ —Ä–∞—Å—Ç–µ–Ω–∏—è –∏ —Ü–≤–µ—Ç—ã       1.0   \n",
            "6  client10022  03.09.2017  sku29930  –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∞–¥–∞ –∏ –¥–∞—á–∏       1.0   \n",
            "7  client10031  13.09.2017  sku12413                 –≠–ª–µ–∫—Ç—Ä–æ—Ç–æ–≤–∞—Ä—ã       7.0   \n",
            "8  client10031  13.09.2017  sku22387                 –õ–∞–∫–∏ –∏ –∫—Ä–∞—Å–∫–∏       1.0   \n",
            "9  client10031  13.09.2017  sku27765             –°—Ç–æ–ª—è—Ä–Ω—ã–µ –∏–∑–¥–µ–ª–∏—è       6.0   \n",
            "\n",
            "   amount  \n",
            "0   992.0  \n",
            "1  1080.0  \n",
            "2    99.0  \n",
            "3    99.0  \n",
            "4    59.0  \n",
            "5    59.0  \n",
            "6    65.0  \n",
            "7  1813.0  \n",
            "8   219.0  \n",
            "9   474.0  \n",
            "\n",
            "–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame: (24908, 7)\n",
            "–†–∞–∑–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ DataFrame: (24801, 6)\n",
            "\n",
            "‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\n",
            "üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\n",
            "   - –ö–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏: aggregated_data\n",
            "   - –ö–∞–∫ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: transactions_aggregated v1.0.0\n",
            "\n",
            "üåê –û—Ç–∫—Ä–æ–π—Ç–µ –≤ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/a97b11d13cd848d6a9a7f3d96132dc4b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (–æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞)**\n",
        "\n",
        "def calculate_client_profile_at_date(transactions_agg_file, observation_end_date='2019-09-01')"
      ],
      "metadata": {
        "id": "kodxe1ZBWKrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º ClearML\n",
        "from clearml import Task, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"taskprep\",  # –î–æ–±–∞–≤–∏–º –¥–∞—Ç—É –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    tags=[\"profile\", \"training_set\"]\n",
        ")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∑–∞–¥–∞—á–∏\n",
        "task.set_base_docker(\"python:3.9\")  # –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "def calculate_client_profile_at_date(transactions_agg_file, observation_end_date='2019-09-01'):\n",
        "    \"\"\"\n",
        "    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—É—é –¥–∞—Ç—É –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑–∏—Ç–æ–≤.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    transactions_agg_file : str\n",
        "        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑–∏—Ç–∞–º–∏ (transactions_aggregated.csv)\n",
        "    observation_end_date : str –∏–ª–∏ datetime-like\n",
        "        –î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è).\n",
        "        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: '2019-09-01' (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ clientID.\n",
        "        –°—Ç–æ–ª–±—Ü—ã: clientID, Recency, Frequency, Monetary, last_visit_date,\n",
        "                 total_quantity, avg_check, total_unique_items,\n",
        "                 avg_items_per_visit, weekend_visits, amount_last_visit\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑–∏—Ç–∞–º–∏\n",
        "    try:\n",
        "        visits_df = pd.read_csv(transactions_agg_file)\n",
        "        print(f\"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(visits_df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {transactions_agg_file}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"–§–∞–π–ª {transactions_agg_file} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    required_columns = ['clientID', 'visit_date', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    missing_columns = [col for col in required_columns if col not in visits_df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {missing_columns}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—É –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ datetime\n",
        "    observation_end_date = pd.to_datetime(observation_end_date)\n",
        "\n",
        "    # 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º visit_date –≤ datetime\n",
        "    visits_df = visits_df.copy()\n",
        "    visits_df['visit_date'] = pd.to_datetime(visits_df['visit_date'])\n",
        "\n",
        "    # 4. –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∏–∑–∏—Ç—ã –°–¢–†–û–ì–û –î–û observation_end_date (–≤–∞–∂–Ω–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö)\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –°–¢–†–û–ì–û–ï –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ: visit_date < observation_end_date\n",
        "    filtered_visits = visits_df[visits_df['visit_date'] < observation_end_date].copy()\n",
        "\n",
        "    if filtered_visits.empty:\n",
        "        print(f\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã {observation_end_date.date()}\")\n",
        "        print(f\"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –≤ –¥–∞–Ω–Ω—ã—Ö: –æ—Ç {visits_df['visit_date'].min().date()} –¥–æ {visits_df['visit_date'].max().date()}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_visits)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    print(f\"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –æ—Ç {filtered_visits['visit_date'].min().date()} \"\n",
        "          f\"–¥–æ {filtered_visits['visit_date'].max().date()} (–Ω–µ –≤–∫–ª—é—á–∞—è {observation_end_date.date()})\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "    unique_clients_before = visits_df['clientID'].nunique()\n",
        "    unique_clients_after = filtered_visits['clientID'].nunique()\n",
        "    print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: –±—ã–ª–æ {unique_clients_before}, —Å—Ç–∞–ª–æ {unique_clients_after}\")\n",
        "\n",
        "    # 5. –°–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame –ø–æ –≤–∏–∑–∏—Ç–∞–º (clientID + visit_date)\n",
        "    visits_agg = filtered_visits.groupby(['clientID', 'visit_date']).agg({\n",
        "        'quantity': 'sum',      # —Å—É–º–º–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –≤–∏–∑–∏—Ç–µ\n",
        "        'amount': 'sum',        # —Å—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫ –≤ –≤–∏–∑–∏—Ç–µ\n",
        "        'item': 'nunique'       # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –≤–∏–∑–∏—Ç–µ\n",
        "    }).reset_index()\n",
        "\n",
        "    # 6. –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –¥–Ω—è (—Å—É–±–±–æ—Ç–∞=5, –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ=6)\n",
        "    visits_agg['is_weekend'] = visits_agg['visit_date'].dt.dayofweek.isin([5, 6])\n",
        "\n",
        "    # 7. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "\n",
        "    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–ª—è RFM –∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    client_stats = visits_agg.groupby('clientID').agg({\n",
        "        'visit_date': ['max', 'nunique'],          # –ø–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç (Frequency)\n",
        "        'amount': 'sum',                           # –æ–±—â–∞—è —Å—É–º–º–∞ (Monetary)\n",
        "        'quantity': 'sum',                         # –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤\n",
        "        'is_weekend': 'sum'                        # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ\n",
        "    })\n",
        "\n",
        "    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º—É–ª—å—Ç–∏–∏–Ω–¥–µ–∫—Å\n",
        "    client_stats.columns = [\n",
        "        'last_visit_date', 'Frequency',\n",
        "        'Monetary', 'total_quantity', 'weekend_visits'\n",
        "    ]\n",
        "\n",
        "    # 8. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Recency (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–Ω–∏–º –≤–∏–∑–∏—Ç–æ–º –∏ observation_end_date)\n",
        "    client_stats['Recency'] = (observation_end_date - client_stats['last_visit_date']).dt.days\n",
        "\n",
        "    # 9. –ü–æ–ª—É—á–∞–µ–º —Å—É–º–º—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    last_visits = visits_agg.sort_values('visit_date').groupby('clientID').last()\n",
        "    client_stats['amount_last_visit'] = last_visits['amount']\n",
        "\n",
        "    # 10. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ –∏—Å—Ç–æ—Ä–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    unique_items_stats = filtered_visits.groupby('clientID').agg({\n",
        "        'item': 'nunique'\n",
        "    }).rename(columns={'item': 'total_unique_items'})\n",
        "\n",
        "    # 11. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "    profiles_df = client_stats.merge(unique_items_stats, left_index=True, right_index=True)\n",
        "\n",
        "    # 12. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    # –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: –æ–±—â–∞—è —Å—É–º–º–∞ / –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑–∏—Ç–æ–≤\n",
        "    profiles_df['avg_check'] = profiles_df['Monetary'] / profiles_df['Frequency']\n",
        "\n",
        "    # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –≤–∏–∑–∏—Ç\n",
        "    profiles_df['avg_items_per_visit'] = profiles_df['total_quantity'] / profiles_df['Frequency']\n",
        "\n",
        "    # 13. –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ Frequency = 0, —Ö–æ—Ç—è —ç—Ç–æ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ)\n",
        "    profiles_df['avg_check'] = profiles_df['avg_check'].fillna(0)\n",
        "    profiles_df['avg_items_per_visit'] = profiles_df['avg_items_per_visit'].fillna(0)\n",
        "\n",
        "    # 14. –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º\n",
        "    profiles_df = profiles_df.reset_index().rename(columns={'index': 'clientID'})\n",
        "\n",
        "    # 15. –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã\n",
        "    column_order = [\n",
        "        'clientID', 'Recency', 'Frequency', 'Monetary',\n",
        "        'last_visit_date', 'total_quantity', 'avg_check',\n",
        "        'total_unique_items', 'avg_items_per_visit',\n",
        "        'weekend_visits', 'amount_last_visit'\n",
        "    ]\n",
        "\n",
        "    profiles_df = profiles_df[column_order]\n",
        "\n",
        "    # 16. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ clientID\n",
        "    profiles_df = profiles_df.sort_values('clientID').reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n–†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–ª—è {len(profiles_df)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "    print(f\"–î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ): {observation_end_date.date()}\")\n",
        "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π:\")\n",
        "    print(f\"- Recency: {profiles_df['Recency'].min()}-{profiles_df['Recency'].max()} –¥–Ω–µ–π \"\n",
        "          f\"(—Å—Ä–µ–¥–Ω–µ–µ: {profiles_df['Recency'].mean():.1f})\")\n",
        "    print(f\"- Frequency: {profiles_df['Frequency'].min()}-{profiles_df['Frequency'].max()} –≤–∏–∑–∏—Ç–æ–≤ \"\n",
        "          f\"(—Å—Ä–µ–¥–Ω–µ–µ: {profiles_df['Frequency'].mean():.1f})\")\n",
        "    print(f\"- Monetary: {profiles_df['Monetary'].min():.2f}-{profiles_df['Monetary'].max():.2f} —Ä—É–±. \"\n",
        "          f\"(—Å—Ä–µ–¥–Ω–µ–µ: {profiles_df['Monetary'].mean():.2f})\")\n",
        "\n",
        "    return profiles_df\n",
        "\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "def save_client_profiles(profiles_df, output_file='client_profiles_until_2019-09-01.csv'):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ CSV —Ñ–∞–π–ª.\n",
        "    \"\"\"\n",
        "    if not profiles_df.empty:\n",
        "        profiles_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n–ü—Ä–æ—Ñ–∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file}\")\n",
        "\n",
        "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ\n",
        "        print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(profiles_df)} —Å—Ç—Ä–æ–∫ √ó {len(profiles_df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ 2019-09-01\n",
        "if __name__ == \"__main__\":\n",
        "    # –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑–∏—Ç–∞–º–∏\n",
        "    transactions_agg_file = 'transactions_aggregated.csv'\n",
        "\n",
        "    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –î–û 2019-09-01 (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–†–ê–°–ß–ï–¢ –ü–†–û–§–ò–õ–ï–ô –ö–õ–ò–ï–ù–¢–û–í –î–û 2019-09-01\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client_profiles = calculate_client_profile_at_date(\n",
        "        transactions_agg_file,\n",
        "        observation_end_date='2019-09-01'  # –°—Ç—Ä–æ–≥–æ –¥–æ —ç—Ç–æ–π –¥–∞—Ç—ã\n",
        "    )\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
        "    if not client_profiles.empty:\n",
        "        print(\"\\n–ü–µ—Ä–≤—ã–µ 10 –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "        print(client_profiles.head(10))\n",
        "\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Recency\n",
        "        print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Recency (–¥–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞):\")\n",
        "        recency_bins = pd.cut(client_profiles['Recency'],\n",
        "                             bins=[0, 7, 30, 90, 180, 365, 1000],\n",
        "                             labels=['0-7', '8-30', '31-90', '91-180', '181-365', '>365'])\n",
        "        recency_dist = recency_bins.value_counts().sort_index()\n",
        "        print(recency_dist)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "        save_client_profiles(client_profiles)\n",
        "\n",
        "  # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
        "task.close()"
      ],
      "metadata": {
        "id": "jAPR5hBBThFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24bb95d-5b27-41de-d035-96771298d1f2",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "ClearML Task: created new task id=fb90114267324d53bb0bd48f61fde0ac\n",
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/fb90114267324d53bb0bd48f61fde0ac/output/log\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "============================================================\n",
            "–†–ê–°–ß–ï–¢ –ü–†–û–§–ò–õ–ï–ô –ö–õ–ò–ï–ù–¢–û–í –î–û 2019-09-01\n",
            "============================================================\n",
            "–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ 1003083 –∑–∞–ø–∏—Å–µ–π –∏–∑ transactions_aggregated.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1428418278.py:68: UserWarning:\n",
            "\n",
            "Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å 892864 –∑–∞–ø–∏—Å–µ–π\n",
            "–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –æ—Ç 2017-09-01 –¥–æ 2019-08-31 (–Ω–µ –≤–∫–ª—é—á–∞—è 2019-09-01)\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: –±—ã–ª–æ 42746, —Å—Ç–∞–ª–æ 39906\n",
            "\n",
            "–†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–ª—è 39906 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ): 2019-09-01\n",
            "\n",
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π:\n",
            "- Recency: 1-730 –¥–Ω–µ–π (—Å—Ä–µ–¥–Ω–µ–µ: 190.4)\n",
            "- Frequency: 1-161 –≤–∏–∑–∏—Ç–æ–≤ (—Å—Ä–µ–¥–Ω–µ–µ: 5.8)\n",
            "- Monetary: 10.00-3887309.00 —Ä—É–±. (—Å—Ä–µ–¥–Ω–µ–µ: 14000.75)\n",
            "\n",
            "–ü–µ—Ä–≤—ã–µ 10 –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "      clientID  Recency  Frequency  Monetary last_visit_date  total_quantity  \\\n",
            "0      client1      587          1      2273      2018-01-22               9   \n",
            "1     client10       27          1      4757      2019-08-05               3   \n",
            "2    client100      116          1      7299      2019-05-08               1   \n",
            "3   client1000        8         12     31792      2019-08-24             151   \n",
            "4  client10000      396          1      8495      2018-08-01               5   \n",
            "5  client10001       21          5      1874      2019-08-11              33   \n",
            "6  client10002       94          4      5850      2019-05-30              25   \n",
            "7  client10003        1         39     54360      2019-08-31             577   \n",
            "8  client10004       35         10      7204      2019-07-28              50   \n",
            "9  client10005      116          5      4846      2019-05-08              33   \n",
            "\n",
            "     avg_check  total_unique_items  avg_items_per_visit  weekend_visits  \\\n",
            "0  2273.000000                   4             9.000000               0   \n",
            "1  4757.000000                   3             3.000000               0   \n",
            "2  7299.000000                   1             1.000000               0   \n",
            "3  2649.333333                  28            12.583333               4   \n",
            "4  8495.000000                   5             5.000000               0   \n",
            "5   374.800000                  18             6.600000               4   \n",
            "6  1462.500000                  18             6.250000               2   \n",
            "7  1393.846154                 110            14.794872               5   \n",
            "8   720.400000                  35             5.000000               4   \n",
            "9   969.200000                  22             6.600000               0   \n",
            "\n",
            "   amount_last_visit  \n",
            "0               2273  \n",
            "1               4757  \n",
            "2               7299  \n",
            "3               1746  \n",
            "4               8495  \n",
            "5                179  \n",
            "6                839  \n",
            "7                385  \n",
            "8                198  \n",
            "9                171  \n",
            "\n",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Recency (–¥–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞):\n",
            "Recency\n",
            "0-7        2894\n",
            "8-30       6220\n",
            "31-90      8169\n",
            "91-180     7289\n",
            "181-365    7164\n",
            ">365       8170\n",
            "Name: count, dtype: int64\n",
            "\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: client_profiles_until_2019-09-01.csv\n",
            "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 39906 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π (–æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞)**\n",
        "\n",
        "def mark_events(visits_df, result_start_date, result_end_date)"
      ],
      "metadata": {
        "id": "H-xwDHSWWjpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º ClearML\n",
        "from clearml import Task, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"taskprep\",  # –î–æ–±–∞–≤–∏–º –¥–∞—Ç—É –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    tags=[\"events\", \"training_set\"]\n",
        ")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∑–∞–¥–∞—á–∏\n",
        "task.set_base_docker(\"python:3.9\")  # –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "def mark_events(visits_df, result_start_date, result_end_date):\n",
        "    \"\"\"\n",
        "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –ø–æ—Å–µ—â–∞–ª –ª–∏ –∫–ª–∏–µ–Ω—Ç –º–∞–≥–∞–∑–∏–Ω –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ [result_start, result_end).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    visits_df : pandas.DataFrame\n",
        "        DataFrame —Å –≤–∏–∑–∏—Ç–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤. –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã:\n",
        "        'clientID' (–∏–ª–∏ 'client') –∏ 'visit_date'\n",
        "    result_start_date : str –∏–ª–∏ datetime-like\n",
        "        –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞ (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ).\n",
        "    result_end_date : str –∏–ª–∏ datetime-like\n",
        "        –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞ (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame —Å –¥–≤—É–º—è –∫–æ–ª–æ–Ω–∫–∞–º–∏:\n",
        "        - client: –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã\n",
        "        - event: True –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –ø–æ—Å–µ—Ç–∏–ª –º–∞–≥–∞–∑–∏–Ω –≤ –ø–µ—Ä–∏–æ–¥, –∏–Ω–∞—á–µ False\n",
        "    \"\"\"\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–∞ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏\n",
        "    client_column = None\n",
        "    possible_client_columns = ['clientID', 'client', 'customer_id', 'customerID']\n",
        "\n",
        "    for col in possible_client_columns:\n",
        "        if col in visits_df.columns:\n",
        "            client_column = col\n",
        "            break\n",
        "\n",
        "    if client_column is None:\n",
        "        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∏–º–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–æ—Ö–æ–∂–∏–π —Å—Ç–æ–ª–±–µ—Ü\n",
        "        for col in visits_df.columns:\n",
        "            if 'client' in col.lower() or 'customer' in col.lower():\n",
        "                client_column = col\n",
        "                break\n",
        "\n",
        "    if client_column is None:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤. \"\n",
        "                        \"–û–∂–∏–¥–∞–µ–º—ã–µ –∏–º–µ–Ω–∞: 'clientID', 'client', 'customer_id', 'customerID'\")\n",
        "\n",
        "    if 'visit_date' not in visits_df.columns:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'visit_date'\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ datetime\n",
        "    result_start_date = pd.to_datetime(result_start_date)\n",
        "    result_end_date = pd.to_datetime(result_end_date)\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º visit_date –≤ datetime, –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω\n",
        "    visits_df = visits_df.copy()\n",
        "    if not pd.api.types.is_datetime64_any_dtype(visits_df['visit_date']):\n",
        "        visits_df['visit_date'] = pd.to_datetime(visits_df['visit_date'])\n",
        "\n",
        "    # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    all_clients = visits_df[client_column].unique()\n",
        "    print(f\"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {len(all_clients)}\")\n",
        "\n",
        "    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∏–∑–∏—Ç—ã –≤ –°–¢–†–û–ì–û–ú –¥–∏–∞–ø–∞–∑–æ–Ω–µ:\n",
        "    # visit_date >= result_start_date –ò visit_date < result_end_date\n",
        "    mask = (visits_df['visit_date'] >= result_start_date) & (visits_df['visit_date'] < result_end_date)\n",
        "    period_visits = visits_df[mask].copy()\n",
        "\n",
        "    print(f\"–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å {result_start_date.date()} –ø–æ {result_end_date.date()} (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\")\n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: {len(period_visits)}\")\n",
        "    print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: {period_visits[client_column].nunique()}\")\n",
        "\n",
        "    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –∫–ª–∏–µ–Ω—Ç—ã –∏–º–µ–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –≤–∏–∑–∏—Ç –≤ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥\n",
        "    clients_with_events = period_visits[client_column].unique()\n",
        "\n",
        "    # 4. –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
        "    result_df = pd.DataFrame({\n",
        "        'client': all_clients,\n",
        "        'event': pd.Series(all_clients).isin(clients_with_events).values\n",
        "    })\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    event_counts = result_df['event'].value_counts()\n",
        "    if True in event_counts.index:\n",
        "        true_count = event_counts[True]\n",
        "    else:\n",
        "        true_count = 0\n",
        "\n",
        "    print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\")\n",
        "    print(f\"- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: {true_count} ({true_count/len(result_df)*100:.1f}%)\")\n",
        "    print(f\"- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: {len(result_df) - true_count} ({(len(result_df)-true_count)/len(result_df)*100:.1f}%)\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "def mark_events_fast(visits_df, result_start_date, result_end_date):\n",
        "    \"\"\"\n",
        "    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ mark_events.\n",
        "    \"\"\"\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' not in visits_df.columns and 'client' not in visits_df.columns:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "\n",
        "    client_column = 'clientID' if 'clientID' in visits_df.columns else 'client'\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã\n",
        "    result_start_date = pd.to_datetime(result_start_date)\n",
        "    result_end_date = pd.to_datetime(result_end_date)\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º visit_date\n",
        "    visits_df = visits_df.copy()\n",
        "    visits_df['visit_date'] = pd.to_datetime(visits_df['visit_date'])\n",
        "\n",
        "    # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    all_clients = pd.Series(visits_df[client_column].unique(), name='client')\n",
        "\n",
        "    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∏–∑–∏—Ç—ã –≤ –ø–µ—Ä–∏–æ–¥–µ\n",
        "    period_mask = (visits_df['visit_date'] >= result_start_date) & (visits_df['visit_date'] < result_end_date)\n",
        "\n",
        "    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Å–æ–±—ã—Ç–∏—è–º–∏\n",
        "    clients_with_events = visits_df.loc[period_mask, client_column].unique()\n",
        "\n",
        "    # 4. –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
        "    result_df = pd.DataFrame({\n",
        "        'client': all_clients,\n",
        "        'event': all_clients.isin(clients_with_events)\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏\n",
        "if __name__ == \"__main__\":\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    test_data = pd.DataFrame({\n",
        "        'clientID': ['client1', 'client1', 'client2', 'client3', 'client2', 'client4', 'client1'],\n",
        "        'visit_date': ['2019-09-01', '2019-09-15', '2019-09-05',\n",
        "                      '2019-10-10', '2019-10-20', '2019-11-05', '2019-12-01'],\n",
        "        'item': ['item1', 'item2', 'item1', 'item3', 'item2', 'item1', 'item2'],\n",
        "        'quantity': [1, 2, 3, 1, 2, 1, 1],\n",
        "        'amount': [100, 200, 150, 300, 120, 80, 90]\n",
        "    })\n",
        "\n",
        "    print(\"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:\")\n",
        "    print(test_data)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # –¢–µ—Å—Ç 1: –ü–µ—Ä–∏–æ–¥, –≤ –∫–æ—Ç–æ—Ä–æ–º –µ—Å—Ç—å –≤–∏–∑–∏—Ç—ã\n",
        "    print(\"–¢–µ—Å—Ç 1: –ü–µ—Ä–∏–æ–¥ —Å 2019-09-01 –ø–æ 2019-10-01\")\n",
        "    result1 = mark_events(test_data, '2019-09-01', '2019-10-01')\n",
        "    print(result1)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # –¢–µ—Å—Ç 2: –ü–µ—Ä–∏–æ–¥, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    print(\"–¢–µ—Å—Ç 2: –ü–µ—Ä–∏–æ–¥ —Å 2019-10-01 –ø–æ 2019-11-01\")\n",
        "    result2 = mark_events(test_data, '2019-10-01', '2019-11-01')\n",
        "    print(result2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # –¢–µ—Å—Ç 3: –ü—É—Å—Ç–æ–π –ø–µ—Ä–∏–æ–¥ (–Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤)\n",
        "    print(\"–¢–µ—Å—Ç 3: –ü–µ—Ä–∏–æ–¥ —Å 2020-01-01 –ø–æ 2020-02-01 (–Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤)\")\n",
        "    result3 = mark_events(test_data, '2020-01-01', '2020-02-01')\n",
        "    print(result3)\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    print(\"–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ñ–∞–π–ª–∞ transactions_aggregated.csv:\")\n",
        "\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
        "        visits_df = pd.read_csv('transactions_aggregated.csv')\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Å–µ–Ω—Ç—è–±—Ä—å 2019)\n",
        "        result_start = '2019-09-01'\n",
        "        result_end = '2019-10-01'  # –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ\n",
        "\n",
        "        result = mark_events(visits_df, result_start, result_end)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "        result.to_csv('client_events_september_2019.csv', index=False)\n",
        "        print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ client_events_september_2019.csv\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"–§–∞–π–ª transactions_aggregated.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m81DqLPsFecT",
        "outputId": "7d21f393-d567-4d8a-93a5-acc91b8b84dd",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "ClearML Task: created new task id=7e480cc92801428ba411bde59e5603f5\n",
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/7e480cc92801428ba411bde59e5603f5/output/log\n",
            "–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
            "  clientID  visit_date   item  quantity  amount\n",
            "0  client1  2019-09-01  item1         1     100\n",
            "1  client1  2019-09-15  item2         2     200\n",
            "2  client2  2019-09-05  item1         3     150\n",
            "3  client3  2019-10-10  item3         1     300\n",
            "4  client2  2019-10-20  item2         2     120\n",
            "5  client4  2019-11-05  item1         1      80\n",
            "6  client1  2019-12-01  item2         1      90\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ—Å—Ç 1: –ü–µ—Ä–∏–æ–¥ —Å 2019-09-01 –ø–æ 2019-10-01\n",
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 4\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2019-09-01 –ø–æ 2019-10-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 3\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 2\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 2 (50.0%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 2 (50.0%)\n",
            "    client  event\n",
            "0  client1   True\n",
            "1  client2   True\n",
            "2  client3  False\n",
            "3  client4  False\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ—Å—Ç 2: –ü–µ—Ä–∏–æ–¥ —Å 2019-10-01 –ø–æ 2019-11-01\n",
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 4\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2019-10-01 –ø–æ 2019-11-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 2\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 2\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 2 (50.0%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 2 (50.0%)\n",
            "    client  event\n",
            "0  client1  False\n",
            "1  client2   True\n",
            "2  client3   True\n",
            "3  client4  False\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ—Å—Ç 3: –ü–µ—Ä–∏–æ–¥ —Å 2020-01-01 –ø–æ 2020-02-01 (–Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤)\n",
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 4\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2020-01-01 –ø–æ 2020-02-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 0\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 0\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 0 (0.0%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 4 (100.0%)\n",
            "    client  event\n",
            "0  client1  False\n",
            "1  client2  False\n",
            "2  client3  False\n",
            "3  client4  False\n",
            "\n",
            "==================================================\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ñ–∞–π–ª–∞ transactions_aggregated.csv:\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3840952294.py:79: UserWarning:\n",
            "\n",
            "Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 30738\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2019-09-01 –ø–æ 2019-10-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 39075\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 6364\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 6364 (20.7%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 24374 (79.3%)\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ client_events_september_2019.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ —Å–æ–±—ã—Ç–∏–π (–æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞)**\n",
        "\n",
        "def create_training_sample(profile_df, events_df)"
      ],
      "metadata": {
        "id": "YdQvDYxrW5nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º ClearML\n",
        "from clearml import Task, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"taskprep\",  # –î–æ–±–∞–≤–∏–º –¥–∞—Ç—É –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    tags=[\"join1\", \"training_set1\"]\n",
        ")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∑–∞–¥–∞—á–∏\n",
        "task.set_base_docker(\"python:3.9\")  # –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "def create_training_sample(profile_df, events_df):\n",
        "    \"\"\"\n",
        "    –°–æ–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏—è –≤ –µ–¥–∏–Ω—É—é –≤—ã–±–æ—Ä–∫—É.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    profile_df : pandas.DataFrame\n",
        "        DataFrame —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç calculate_client_profile_at_date).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'.\n",
        "    events_df : pandas.DataFrame\n",
        "        DataFrame —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏–π (—Ä–µ–∑—É–ª—å—Ç–∞—Ç mark_events).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã 'client' –∏ 'event'.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã\n",
        "    profile = profile_df.copy()\n",
        "    events = events_df.copy()\n",
        "\n",
        "    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' not in profile.columns:\n",
        "        raise ValueError(\"profile_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'\")\n",
        "\n",
        "    if 'client' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'client'\")\n",
        "\n",
        "    if 'event' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'event'\")\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü –≤ profile –¥–ª—è join (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' in profile.columns and 'client' in events.columns:\n",
        "        print(f\"–ü—Ä–æ—Ñ–∏–ª–∏: {len(profile)} –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: {len(events)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: {list(profile.columns)}\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: {list(events.columns)}\")\n",
        "\n",
        "        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –¥–ª—è join\n",
        "        profile = profile.rename(columns={'clientID': 'client'})\n",
        "\n",
        "    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø—Ä–æ—Ñ–∏–ª—è –∏ —Å–æ–±—ã—Ç–∏–π –ø–æ 'client'\n",
        "    print(f\"\\n1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\")\n",
        "    before_join = len(profile)\n",
        "\n",
        "    training_sample = pd.merge(\n",
        "        profile,\n",
        "        events[['client', 'event']],  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n",
        "        on='client',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    after_join = len(training_sample)\n",
        "    print(f\"   –î–æ join: {before_join} –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\")\n",
        "    print(f\"   –ü–æ—Å–ª–µ inner join: {after_join} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "    print(f\"   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {before_join - after_join} (–Ω–µ—Ç –≤ events_df)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–∞ –ª–∏ –≤—ã–±–æ—Ä–∫–∞\n",
        "    if training_sample.empty:\n",
        "        raise ValueError(\"–†–µ–∑—É–ª—å—Ç–∞—Ç inner join –ø—É—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤.\")\n",
        "\n",
        "    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'event' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "    print(f\"\\n2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\")\n",
        "    missing_events = training_sample['event'].isnull().sum()\n",
        "\n",
        "    if missing_events > 0:\n",
        "        print(f\"   –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {missing_events} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π!\")\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–µ–º False\")\n",
        "        training_sample['event'] = training_sample['event'].fillna(False)\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    if training_sample['event'].dtype != 'bool':\n",
        "        print(f\"   –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 'event' –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø\")\n",
        "        training_sample['event'] = training_sample['event'].astype(bool)\n",
        "\n",
        "    # 4. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏ (–æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
        "    print(f\"\\n3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\")\n",
        "\n",
        "    date_columns = []\n",
        "    for col in training_sample.columns:\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–æ–ª–±–µ—Ü datetime –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 'date' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏\n",
        "        if (pd.api.types.is_datetime64_any_dtype(training_sample[col]) or\n",
        "            'date' in col.lower() or\n",
        "            col.lower() in ['last_visit_date', 'visit_date']):\n",
        "            date_columns.append(col)\n",
        "\n",
        "    if date_columns:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: {date_columns}\")\n",
        "        training_sample = training_sample.drop(columns=date_columns)\n",
        "    else:\n",
        "        print(f\"   –°—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "\n",
        "    # 5. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –≤—ã–±–æ—Ä–∫–µ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Å–æ–±—ã—Ç–∏—è (True –∏ False)\n",
        "    print(f\"\\n4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\")\n",
        "\n",
        "    class_counts = training_sample['event'].value_counts()\n",
        "    print(f\"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    for value, count in class_counts.items():\n",
        "        percentage = count / len(training_sample) * 100\n",
        "        print(f\"   - event={value}: {count} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤\n",
        "    unique_classes = training_sample['event'].nunique()\n",
        "    if unique_classes < 2:\n",
        "        raise ValueError(f\"–í –≤—ã–±–æ—Ä–∫–µ —Ç–æ–ª—å–∫–æ {unique_classes} –∫–ª–∞—Å—Å! –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –æ–±–∞ –∫–ª–∞—Å—Å–∞ (True –∏ False).\")\n",
        "\n",
        "    # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "    print(f\"\\n5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\")\n",
        "    missing_values = training_sample.isnull().sum()\n",
        "    columns_with_missing = missing_values[missing_values > 0]\n",
        "\n",
        "    if len(columns_with_missing) > 0:\n",
        "        print(f\"   –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö:\")\n",
        "        for col, count in columns_with_missing.items():\n",
        "            percentage = count / len(training_sample) * 100\n",
        "            print(f\"   - {col}: {count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏...\")\n",
        "        for col in columns_with_missing.index:\n",
        "            if pd.api.types.is_numeric_dtype(training_sample[col]):\n",
        "                median_val = training_sample[col].median()\n",
        "                training_sample[col] = training_sample[col].fillna(median_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–µ–¥–∏–∞–Ω–æ–π ({median_val:.2f})\")\n",
        "            else:\n",
        "                # –î–ª—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º —Å–∞–º—ã–º —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "                mode_val = training_sample[col].mode()[0] if not training_sample[col].mode().empty else 'Unknown'\n",
        "                training_sample[col] = training_sample[col].fillna(mode_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–æ–¥–æ–π ({mode_val})\")\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\")\n",
        "\n",
        "    # 7. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "    print(f\"\\n6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä: {len(training_sample)} —Å—Ç—Ä–æ–∫ √ó {len(training_sample.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    print(f\"   –°—Ç–æ–ª–±—Ü—ã: {list(training_sample.columns)}\")\n",
        "\n",
        "    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    print(f\"   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    for col in training_sample.columns:\n",
        "        dtype = training_sample[col].dtype\n",
        "        if col == 'event':\n",
        "            print(f\"   - {col}: {dtype} (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\")\n",
        "        elif col == 'client':\n",
        "            print(f\"   - {col}: {dtype} (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\")\n",
        "        else:\n",
        "            print(f\"   - {col}: {dtype}\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
        "    numeric_cols = training_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'event' in numeric_cols:\n",
        "        numeric_cols.remove('event')\n",
        "\n",
        "    if numeric_cols:\n",
        "        print(f\"\\n   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\")\n",
        "        for col in numeric_cols[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏\n",
        "            if col in training_sample.columns:\n",
        "                print(f\"   - {col}: min={training_sample[col].min():.2f}, \"\n",
        "                      f\"max={training_sample[col].max():.2f}, \"\n",
        "                      f\"mean={training_sample[col].mean():.2f}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return training_sample\n",
        "\n",
        "\n",
        "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏\n",
        "def save_training_sample(training_sample, output_file='training_sample.csv'):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –≤ CSV —Ñ–∞–π–ª.\n",
        "    \"\"\"\n",
        "    if not training_sample.empty:\n",
        "        training_sample.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_file}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –≤–µ—Ä—Å–∏—é –±–µ–∑ client –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "        features_sample = training_sample.drop(columns=['client'] if 'client' in training_sample.columns else [])\n",
        "        features_file = 'training_sample_features.csv'\n",
        "        features_sample.to_csv(features_file, index=False, encoding='utf-8')\n",
        "        print(f\"–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {features_file}\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    test_profiles = pd.DataFrame({\n",
        "        'clientID': ['client1', 'client2', 'client3', 'client4', 'client5'],\n",
        "        'Recency': [15, 45, 120, 7, 90],\n",
        "        'Frequency': [5, 2, 1, 10, 3],\n",
        "        'Monetary': [1500, 800, 300, 5000, 1200],\n",
        "        'last_visit_date': ['2019-08-15', '2019-07-15', '2019-05-01', '2019-08-24', '2019-06-01'],\n",
        "        'total_quantity': [25, 10, 3, 50, 15],\n",
        "        'avg_check': [300, 400, 300, 500, 400],\n",
        "        'total_unique_items': [8, 5, 2, 15, 6],\n",
        "        'avg_items_per_visit': [5, 5, 3, 5, 5],\n",
        "        'weekend_visits': [2, 1, 0, 4, 1],\n",
        "        'amount_last_visit': [350, 450, 300, 600, 420]\n",
        "    })\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π\n",
        "    test_events = pd.DataFrame({\n",
        "        'client': ['client1', 'client2', 'client3', 'client4', 'client6'],  # client5 –Ω–µ—Ç, client6 –Ω–µ—Ç –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
        "        'event': [True, False, True, True, False]\n",
        "    })\n",
        "\n",
        "    print(\"–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\")\n",
        "    print(\"–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "    print(test_profiles)\n",
        "    print(\"\\n–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\")\n",
        "    print(test_events)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "    try:\n",
        "        training_sample = create_training_sample(test_profiles, test_events)\n",
        "\n",
        "        print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\")\n",
        "        print(training_sample)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∫–∏: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç)\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–æ 2019-09-01)\n",
        "        profiles = pd.read_csv('client_profiles_until_2019-09-01.csv')\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ —Å–µ–Ω—Ç—è–±—Ä—å 2019)\n",
        "        events = pd.read_csv('client_events_september_2019.csv')\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "        real_training_sample = create_training_sample(profiles, events)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "        save_training_sample(real_training_sample, 'train_sample.csv')\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {e}\")\n",
        "        print(\"–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\")\n",
        "        print(\"1. calculate_client_profile_at_date() - –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π\")\n",
        "        print(\"2. mark_events() - –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π\")\n",
        "\n",
        "  # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
        "task.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "xIp1UTbYMrk7",
        "outputId": "8d764aca-c19e-48bf-8ec9-193364a7d9a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "  clientID  Recency  Frequency  Monetary last_visit_date  total_quantity  \\\n",
            "0  client1       15          5      1500      2019-08-15              25   \n",
            "1  client2       45          2       800      2019-07-15              10   \n",
            "2  client3      120          1       300      2019-05-01               3   \n",
            "3  client4        7         10      5000      2019-08-24              50   \n",
            "4  client5       90          3      1200      2019-06-01              15   \n",
            "\n",
            "   avg_check  total_unique_items  avg_items_per_visit  weekend_visits  \\\n",
            "0        300                   8                    5               2   \n",
            "1        400                   5                    5               1   \n",
            "2        300                   2                    3               0   \n",
            "3        500                  15                    5               4   \n",
            "4        400                   6                    5               1   \n",
            "\n",
            "   amount_last_visit  \n",
            "0                350  \n",
            "1                450  \n",
            "2                300  \n",
            "3                600  \n",
            "4                420  \n",
            "\n",
            "–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\n",
            "    client  event\n",
            "0  client1   True\n",
            "1  client2  False\n",
            "2  client3   True\n",
            "3  client4   True\n",
            "4  client6  False\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 1 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=True: 3 –∫–ª–∏–µ–Ω—Ç–æ–≤ (75.0%)\n",
            "   - event=False: 1 –∫–ª–∏–µ–Ω—Ç–æ–≤ (25.0%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 4 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: int64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: int64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=7.00, max=120.00, mean=46.75\n",
            "   - Frequency: min=1.00, max=10.00, mean=4.50\n",
            "   - Monetary: min=300.00, max=5000.00, mean=1900.00\n",
            "   - total_quantity: min=3.00, max=50.00, mean=22.00\n",
            "   - avg_check: min=300.00, max=500.00, mean=375.00\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\n",
            "    client  Recency  Frequency  Monetary  total_quantity  avg_check  \\\n",
            "0  client1       15          5      1500              25        300   \n",
            "1  client2       45          2       800              10        400   \n",
            "2  client3      120          1       300               3        300   \n",
            "3  client4        7         10      5000              50        500   \n",
            "\n",
            "   total_unique_items  avg_items_per_visit  weekend_visits  amount_last_visit  \\\n",
            "0                   8                    5               2                350   \n",
            "1                   5                    5               1                450   \n",
            "2                   2                    3               0                300   \n",
            "3                  15                    5               4                600   \n",
            "\n",
            "   event  \n",
            "0   True  \n",
            "1  False  \n",
            "2   True  \n",
            "3   True  \n",
            "\n",
            "============================================================\n",
            "–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\n",
            "============================================================\n",
            "–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: [Errno 2] No such file or directory: 'client_profiles_until_2019-09-01.csv'\n",
            "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\n",
            "1. calculate_client_profile_at_date() - –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π\n",
            "2. mark_events() - –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "# 1. –ü–æ–¥–∫–ª—é—á–∞–µ–º ClearML\n",
        "from clearml import Task, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"taskprep\",  # –î–æ–±–∞–≤–∏–º –¥–∞—Ç—É –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    tags=[\"join\", \"training_set\"]\n",
        ")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –∑–∞–¥–∞—á–∏\n",
        "task.set_base_docker(\"python:3.9\")  # –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "def create_training_sample(profile_df, events_df):\n",
        "    \"\"\"\n",
        "    –°–æ–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏—è –≤ –µ–¥–∏–Ω—É—é –≤—ã–±–æ—Ä–∫—É.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    profile_df : pandas.DataFrame\n",
        "        DataFrame —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç calculate_client_profile_at_date).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'.\n",
        "    events_df : pandas.DataFrame\n",
        "        DataFrame —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏–π (—Ä–µ–∑—É–ª—å—Ç–∞—Ç mark_events).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã 'client' –∏ 'event'.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã\n",
        "    profile = profile_df.copy()\n",
        "    events = events_df.copy()\n",
        "\n",
        "    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' not in profile.columns:\n",
        "        raise ValueError(\"profile_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'\")\n",
        "\n",
        "    if 'client' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'client'\")\n",
        "\n",
        "    if 'event' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'event'\")\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü –≤ profile –¥–ª—è join (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' in profile.columns and 'client' in events.columns:\n",
        "        print(f\"–ü—Ä–æ—Ñ–∏–ª–∏: {len(profile)} –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: {len(events)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: {list(profile.columns)}\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: {list(events.columns)}\")\n",
        "\n",
        "        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –¥–ª—è join\n",
        "        profile = profile.rename(columns={'clientID': 'client'})\n",
        "\n",
        "    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø—Ä–æ—Ñ–∏–ª—è –∏ —Å–æ–±—ã—Ç–∏–π –ø–æ 'client'\n",
        "    print(f\"\\n1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\")\n",
        "    before_join = len(profile)\n",
        "\n",
        "    training_sample = pd.merge(\n",
        "        profile,\n",
        "        events[['client', 'event']],  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n",
        "        on='client',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    after_join = len(training_sample)\n",
        "    print(f\"   –î–æ join: {before_join} –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\")\n",
        "    print(f\"   –ü–æ—Å–ª–µ inner join: {after_join} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "    print(f\"   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {before_join - after_join} (–Ω–µ—Ç –≤ events_df)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–∞ –ª–∏ –≤—ã–±–æ—Ä–∫–∞\n",
        "    if training_sample.empty:\n",
        "        raise ValueError(\"–†–µ–∑—É–ª—å—Ç–∞—Ç inner join –ø—É—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤.\")\n",
        "\n",
        "    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'event' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "    print(f\"\\n2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\")\n",
        "    missing_events = training_sample['event'].isnull().sum()\n",
        "\n",
        "    if missing_events > 0:\n",
        "        print(f\"   –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {missing_events} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π!\")\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–µ–º False\")\n",
        "        training_sample['event'] = training_sample['event'].fillna(False)\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    if training_sample['event'].dtype != 'bool':\n",
        "        print(f\"   –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 'event' –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø\")\n",
        "        training_sample['event'] = training_sample['event'].astype(bool)\n",
        "\n",
        "    # 4. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏ (–æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
        "    print(f\"\\n3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\")\n",
        "\n",
        "    date_columns = []\n",
        "    for col in training_sample.columns:\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–æ–ª–±–µ—Ü datetime –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 'date' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏\n",
        "        if (pd.api.types.is_datetime64_any_dtype(training_sample[col]) or\n",
        "            'date' in col.lower() or\n",
        "            col.lower() in ['last_visit_date', 'visit_date']):\n",
        "            date_columns.append(col)\n",
        "\n",
        "    if date_columns:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: {date_columns}\")\n",
        "        training_sample = training_sample.drop(columns=date_columns)\n",
        "    else:\n",
        "        print(f\"   –°—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "\n",
        "    # 5. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –≤—ã–±–æ—Ä–∫–µ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Å–æ–±—ã—Ç–∏—è (True –∏ False)\n",
        "    print(f\"\\n4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\")\n",
        "\n",
        "    class_counts = training_sample['event'].value_counts()\n",
        "    print(f\"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    for value, count in class_counts.items():\n",
        "        percentage = count / len(training_sample) * 100\n",
        "        print(f\"   - event={value}: {count} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤\n",
        "    unique_classes = training_sample['event'].nunique()\n",
        "    if unique_classes < 2:\n",
        "        raise ValueError(f\"–í –≤—ã–±–æ—Ä–∫–µ —Ç–æ–ª—å–∫–æ {unique_classes} –∫–ª–∞—Å—Å! –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –æ–±–∞ –∫–ª–∞—Å—Å–∞ (True –∏ False).\")\n",
        "\n",
        "    # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "    print(f\"\\n5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\")\n",
        "    missing_values = training_sample.isnull().sum()\n",
        "    columns_with_missing = missing_values[missing_values > 0]\n",
        "\n",
        "    if len(columns_with_missing) > 0:\n",
        "        print(f\"   –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö:\")\n",
        "        for col, count in columns_with_missing.items():\n",
        "            percentage = count / len(training_sample) * 100\n",
        "            print(f\"   - {col}: {count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏...\")\n",
        "        for col in columns_with_missing.index:\n",
        "            if pd.api.types.is_numeric_dtype(training_sample[col]):\n",
        "                median_val = training_sample[col].median()\n",
        "                training_sample[col] = training_sample[col].fillna(median_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–µ–¥–∏–∞–Ω–æ–π ({median_val:.2f})\")\n",
        "            else:\n",
        "                # –î–ª—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º —Å–∞–º—ã–º —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "                mode_val = training_sample[col].mode()[0] if not training_sample[col].mode().empty else 'Unknown'\n",
        "                training_sample[col] = training_sample[col].fillna(mode_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–æ–¥–æ–π ({mode_val})\")\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\")\n",
        "\n",
        "    # 7. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "    print(f\"\\n6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä: {len(training_sample)} —Å—Ç—Ä–æ–∫ √ó {len(training_sample.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    print(f\"   –°—Ç–æ–ª–±—Ü—ã: {list(training_sample.columns)}\")\n",
        "\n",
        "    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    print(f\"   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    for col in training_sample.columns:\n",
        "        dtype = training_sample[col].dtype\n",
        "        if col == 'event':\n",
        "            print(f\"   - {col}: {dtype} (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\")\n",
        "        elif col == 'client':\n",
        "            print(f\"   - {col}: {dtype} (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\")\n",
        "        else:\n",
        "            print(f\"   - {col}: {dtype}\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
        "    numeric_cols = training_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'event' in numeric_cols:\n",
        "        numeric_cols.remove('event')\n",
        "\n",
        "    if numeric_cols:\n",
        "        print(f\"\\n   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\")\n",
        "        for col in numeric_cols[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏\n",
        "            if col in training_sample.columns:\n",
        "                print(f\"   - {col}: min={training_sample[col].min():.2f}, \"\n",
        "                      f\"max={training_sample[col].max():.2f}, \"\n",
        "                      f\"mean={training_sample[col].mean():.2f}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return training_sample\n",
        "\n",
        "\n",
        "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏\n",
        "def save_training_sample(training_sample, output_file='training_sample.csv'):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –≤ CSV —Ñ–∞–π–ª.\n",
        "    \"\"\"\n",
        "    if not training_sample.empty:\n",
        "        training_sample.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_file}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –≤–µ—Ä—Å–∏—é –±–µ–∑ client –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "        features_sample = training_sample.drop(columns=['client'] if 'client' in training_sample.columns else [])\n",
        "        features_file = 'training_sample_features.csv'\n",
        "        features_sample.to_csv(features_file, index=False, encoding='utf-8')\n",
        "        print(f\"–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {features_file}\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –≤ ClearML –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∏ –¥–∞—Ç–∞—Å–µ—Ç\n",
        "def save_to_clearml(file_path, dataset_name=\"training_sample\"):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª –≤ ClearML –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
        "    dataset_name : str\n",
        "        –ò–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ ClearML\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –í CLEARML\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
        "        return None, None\n",
        "\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(f\"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data)} —Å—Ç—Ä–æ–∫, {len(data.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏\n",
        "    print(\"\\n1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏...\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
        "    metadata = {\n",
        "        \"file_name\": file_path.name,\n",
        "        \"file_size_bytes\": os.path.getsize(file_path),\n",
        "        \"rows\": len(data),\n",
        "        \"columns\": list(data.columns),\n",
        "        \"creation_date\": datetime.datetime.now().isoformat(),\n",
        "        \"file_path\": str(file_path.absolute())\n",
        "    }\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–±—ã—Ç–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–∫–æ–π —Å—Ç–æ–ª–±–µ—Ü\n",
        "    if 'event' in data.columns:\n",
        "        event_true = int(data['event'].sum())\n",
        "        event_false = int((~data['event']).sum())\n",
        "        metadata[\"event_distribution\"] = {\n",
        "            \"True\": event_true,\n",
        "            \"False\": event_false,\n",
        "            \"true_percentage\": float(event_true/len(data)*100) if len(data) > 0 else 0,\n",
        "            \"false_percentage\": float(event_false/len(data)*100) if len(data) > 0 else 0\n",
        "        }\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç\n",
        "    task.upload_artifact(\n",
        "        name=\"training_sample\",\n",
        "        artifact_object=str(file_path),\n",
        "        metadata=metadata\n",
        "    )\n",
        "\n",
        "    print(f\"   ‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç '{file_path.name}' –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
        "\n",
        "    # 2. –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    print(\"\\n2. –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...\")\n",
        "\n",
        "    # –ì–æ—Ç–æ–≤–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "    description = f\"\"\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤.\n",
        "–§–∞–π–ª: {file_path.name}\n",
        "–°–æ–∑–¥–∞–Ω–æ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "–†–∞–∑–º–µ—Ä: {len(data)} —Å—Ç—Ä–æ–∫ √ó {len(data.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "–°—Ç–æ–ª–±—Ü—ã: {', '.join(data.columns)}\n",
        "\"\"\"\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "    if 'event' in data.columns:\n",
        "        event_true = data['event'].sum()\n",
        "        event_false = (~data['event']).sum()\n",
        "        total = len(data)\n",
        "\n",
        "        description += f\"\"\"\n",
        "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
        "  - True: {event_true} ({event_true/total*100:.1f}%)\n",
        "  - False: {event_false} ({event_false/total*100:.1f}%)\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "        dataset = Dataset.create(\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_project=\"CourseInz/Datasets\",\n",
        "            dataset_version=None,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ—Ä—Å–∏—è\n",
        "            description=description,\n",
        "            parent_datasets=None\n",
        "        )\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª\n",
        "        dataset.add_files(path=str(file_path))\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫\n",
        "        output_dir = Path(\"clearml_temp\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        columns_info = pd.DataFrame({\n",
        "            'column': data.columns,\n",
        "            'dtype': data.dtypes.astype(str),\n",
        "            'non_null': data.notnull().sum().values,\n",
        "            'null': data.isnull().sum().values,\n",
        "            'description': [\n",
        "                '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞' if col == 'client' else\n",
        "                '–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –ø–æ—Å–µ—â–µ–Ω–∏–µ –≤ –ø–µ—Ä–∏–æ–¥' if col == 'event' else\n",
        "                f'–ü—Ä–∏–∑–Ω–∞–∫: {col}'\n",
        "                for col in data.columns\n",
        "            ]\n",
        "        })\n",
        "        columns_file = output_dir / \"columns_description.csv\"\n",
        "        columns_info.to_csv(columns_file, index=False)\n",
        "        dataset.add_files(path=str(columns_file))\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
        "        metadata_file = output_dir / \"dataset_metadata.json\"\n",
        "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "        dataset.add_files(path=str(metadata_file))\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML\n",
        "        print(\"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML...\")\n",
        "        dataset.upload()\n",
        "\n",
        "        # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
        "        dataset.finalize()\n",
        "\n",
        "        print(f\"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {dataset.name}, –≤–µ—Ä—Å–∏—è: {dataset.version}\")\n",
        "        print(f\"   üìä ID –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset.id}\")\n",
        "\n",
        "        # 3. –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤ –∑–∞–¥–∞—á–µ\n",
        "        task.set_parameter(\"dataset_id\", dataset.id)\n",
        "        task.set_parameter(\"dataset_name\", dataset.name)\n",
        "        task.set_parameter(\"dataset_version\", dataset.version)\n",
        "\n",
        "        # 4. –õ–æ–≥–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–∏–º–µ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö\n",
        "        logger = task.get_logger()\n",
        "        logger.report_table(\n",
        "            title=f\"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {file_path.name} (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫)\",\n",
        "            series=\"training_sample\",\n",
        "            table_plot=data.head(10),\n",
        "            iteration=0\n",
        "        )\n",
        "\n",
        "        # 5. –õ–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "        logger.report_single_value(\n",
        "            name=\"training_sample_rows\",\n",
        "            value=len(data)\n",
        "        )\n",
        "        logger.report_single_value(\n",
        "            name=\"training_sample_columns\",\n",
        "            value=len(data.columns)\n",
        "        )\n",
        "\n",
        "        if 'event' in data.columns:\n",
        "            event_true = data['event'].sum()\n",
        "            event_false = (~data['event']).sum()\n",
        "            total = len(data)\n",
        "\n",
        "            logger.report_single_value(\n",
        "                name=\"event_true_count\",\n",
        "                value=int(event_true)\n",
        "            )\n",
        "            logger.report_single_value(\n",
        "                name=\"event_false_count\",\n",
        "                value=int(event_false)\n",
        "            )\n",
        "\n",
        "            if total > 0:\n",
        "                logger.report_single_value(\n",
        "                    name=\"event_true_percentage\",\n",
        "                    value=float(event_true/total*100)\n",
        "                )\n",
        "                logger.report_single_value(\n",
        "                    name=\"event_false_percentage\",\n",
        "                    value=float(event_false/total*100)\n",
        "                )\n",
        "\n",
        "        print(\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ClearML:\")\n",
        "        print(f\"   üìÅ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç: training_sample\")\n",
        "        print(f\"   üóÇÔ∏è  –î–∞—Ç–∞—Å–µ—Ç: {dataset.name} v{dataset.version}\")\n",
        "        print(f\"   üîó –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")\n",
        "\n",
        "        return dataset, file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ ClearML: {e}\")\n",
        "        return None, file_path\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    test_profiles = pd.DataFrame({\n",
        "        'clientID': ['client1', 'client2', 'client3', 'client4', 'client5'],\n",
        "        'Recency': [15, 45, 120, 7, 90],\n",
        "        'Frequency': [5, 2, 1, 10, 3],\n",
        "        'Monetary': [1500, 800, 300, 5000, 1200],\n",
        "        'last_visit_date': ['2019-08-15', '2019-07-15', '2019-05-01', '2019-08-24', '2019-06-01'],\n",
        "        'total_quantity': [25, 10, 3, 50, 15],\n",
        "        'avg_check': [300, 400, 300, 500, 400],\n",
        "        'total_unique_items': [8, 5, 2, 15, 6],\n",
        "        'avg_items_per_visit': [5, 5, 3, 5, 5],\n",
        "        'weekend_visits': [2, 1, 0, 4, 1],\n",
        "        'amount_last_visit': [350, 450, 300, 600, 420]\n",
        "    })\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π\n",
        "    test_events = pd.DataFrame({\n",
        "        'client': ['client1', 'client2', 'client3', 'client4', 'client6'],  # client5 –Ω–µ—Ç, client6 –Ω–µ—Ç –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
        "        'event': [True, False, True, True, False]\n",
        "    })\n",
        "\n",
        "    print(\"–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\")\n",
        "    print(\"–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "    print(test_profiles)\n",
        "    print(\"\\n–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\")\n",
        "    print(test_events)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "    try:\n",
        "        training_sample = create_training_sample(test_profiles, test_events)\n",
        "\n",
        "        print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\")\n",
        "        print(training_sample)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∫–∏: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç)\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–æ 2019-09-01)\n",
        "        profiles = pd.read_csv('client_profiles_until_2019-09-01.csv')\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ —Å–µ–Ω—Ç—è–±—Ä—å 2019)\n",
        "        events = pd.read_csv('client_events_september_2019.csv')\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "        real_training_sample = create_training_sample(profiles, events)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "        save_training_sample(real_training_sample, 'train_sample.csv')\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤ ClearML\n",
        "        dataset, saved_file = save_to_clearml('train_sample.csv', dataset_name=\"training_sample_september_2019\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {e}\")\n",
        "        print(\"–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\")\n",
        "        print(\"1. calculate_client_profile_at_date() - –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π\")\n",
        "        print(\"2. mark_events() - –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∫–∏: {e}\")\n",
        "\n",
        "# –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\n",
        "task.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Oy6ZCuUn9Nqc",
        "outputId": "ebbd58dd-5622-4879-902d-47212a78f686"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "ClearML Task: created new task id=5c2bb41c9dec49ab9bd524e764839dd4\n",
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/5c2bb41c9dec49ab9bd524e764839dd4/output/log\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "  clientID  Recency  Frequency  Monetary last_visit_date  total_quantity  \\\n",
            "0  client1       15          5      1500      2019-08-15              25   \n",
            "1  client2       45          2       800      2019-07-15              10   \n",
            "2  client3      120          1       300      2019-05-01               3   \n",
            "3  client4        7         10      5000      2019-08-24              50   \n",
            "4  client5       90          3      1200      2019-06-01              15   \n",
            "\n",
            "   avg_check  total_unique_items  avg_items_per_visit  weekend_visits  \\\n",
            "0        300                   8                    5               2   \n",
            "1        400                   5                    5               1   \n",
            "2        300                   2                    3               0   \n",
            "3        500                  15                    5               4   \n",
            "4        400                   6                    5               1   \n",
            "\n",
            "   amount_last_visit  \n",
            "0                350  \n",
            "1                450  \n",
            "2                300  \n",
            "3                600  \n",
            "4                420  \n",
            "\n",
            "–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\n",
            "    client  event\n",
            "0  client1   True\n",
            "1  client2  False\n",
            "2  client3   True\n",
            "3  client4   True\n",
            "4  client6  False\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 1 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=True: 3 –∫–ª–∏–µ–Ω—Ç–æ–≤ (75.0%)\n",
            "   - event=False: 1 –∫–ª–∏–µ–Ω—Ç–æ–≤ (25.0%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 4 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: int64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: int64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=7.00, max=120.00, mean=46.75\n",
            "   - Frequency: min=1.00, max=10.00, mean=4.50\n",
            "   - Monetary: min=300.00, max=5000.00, mean=1900.00\n",
            "   - total_quantity: min=3.00, max=50.00, mean=22.00\n",
            "   - avg_check: min=300.00, max=500.00, mean=375.00\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\n",
            "    client  Recency  Frequency  Monetary  total_quantity  avg_check  \\\n",
            "0  client1       15          5      1500              25        300   \n",
            "1  client2       45          2       800              10        400   \n",
            "2  client3      120          1       300               3        300   \n",
            "3  client4        7         10      5000              50        500   \n",
            "\n",
            "   total_unique_items  avg_items_per_visit  weekend_visits  amount_last_visit  \\\n",
            "0                   8                    5               2                350   \n",
            "1                   5                    5               1                450   \n",
            "2                   2                    3               0                300   \n",
            "3                  15                    5               4                600   \n",
            "\n",
            "   event  \n",
            "0   True  \n",
            "1  False  \n",
            "2   True  \n",
            "3   True  \n",
            "\n",
            "============================================================\n",
            "–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\n",
            "============================================================\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 39906 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 30738 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 39906 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 28701 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 11205 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=False: 23271 –∫–ª–∏–µ–Ω—Ç–æ–≤ (81.1%)\n",
            "   - event=True: 5430 –∫–ª–∏–µ–Ω—Ç–æ–≤ (18.9%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 28701 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: float64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: float64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=1.00, max=730.00, mean=190.36\n",
            "   - Frequency: min=1.00, max=161.00, mean=5.78\n",
            "   - Monetary: min=10.00, max=3887309.00, mean=13974.10\n",
            "   - total_quantity: min=1.00, max=5116.00, mean=49.91\n",
            "   - avg_check: min=10.00, max=125860.00, mean=2964.23\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: train_sample.csv\n",
            "–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: training_sample_features.csv\n",
            "\n",
            "============================================================\n",
            "–°–û–•–†–ê–ù–ï–ù–ò–ï –í CLEARML\n",
            "============================================================\n",
            "‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: 28701 —Å—Ç—Ä–æ–∫, 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "\n",
            "1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏...\n",
            "   ‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç 'train_sample.csv' –∑–∞–≥—Ä—É–∂–µ–Ω\n",
            "\n",
            "2. –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...\n",
            "ClearML results page: https://app.clear.ml/projects/d3aace434a064f76a4bd72bbe0263e37/experiments/da192194265543feaaf7c1dd5045e35e/output/log\n",
            "ClearML dataset page: https://app.clear.ml/datasets/simple/d3aace434a064f76a4bd72bbe0263e37/experiments/da192194265543feaaf7c1dd5045e35e\n",
            "   –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML...\n",
            "Uploading dataset changes (3 files compressed to 527.91 KiB) to https://files.clear.ml\n",
            "File compression and upload completed: total size 527.91 KiB, 1 chunk(s) stored (average size 527.91 KiB)\n",
            "   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: training_sample_september_2019, –≤–µ—Ä—Å–∏—è: 1.0.0\n",
            "   üìä ID –¥–∞—Ç–∞—Å–µ—Ç–∞: da192194265543feaaf7c1dd5045e35e\n",
            "\n",
            "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ClearML:\n",
            "   üìÅ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç: training_sample\n",
            "   üóÇÔ∏è  –î–∞—Ç–∞—Å–µ—Ç: training_sample_september_2019 v1.0.0\n",
            "   üîó –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/5c2bb41c9dec49ab9bd524e764839dd4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (–¢–ï–°–¢–û–í–ê–Ø –≤—ã–±–æ—Ä–∫–∞)**\n",
        "\n",
        "def calculate_client_profile_at_date(transactions_agg_file, observation_end_date='2019-10-01')"
      ],
      "metadata": {
        "id": "JpggEmSNYvG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "def calculate_client_profile_at_date(transactions_agg_file, observation_end_date='2019-10-01'):\n",
        "    \"\"\"\n",
        "    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—É—é –¥–∞—Ç—É –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "    –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–∑–∏—Ç–æ–≤.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    transactions_agg_file : str\n",
        "        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑–∏—Ç–∞–º–∏ (transactions_aggregated.csv)\n",
        "    observation_end_date : str –∏–ª–∏ datetime-like\n",
        "        –î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è).\n",
        "        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: '2019-10-01' (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ clientID.\n",
        "        –°—Ç–æ–ª–±—Ü—ã: clientID, Recency, Frequency, Monetary, last_visit_date,\n",
        "                 total_quantity, avg_check, total_unique_items,\n",
        "                 avg_items_per_visit, weekend_visits, amount_last_visit\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑–∏—Ç–∞–º–∏\n",
        "    try:\n",
        "        visits_df = pd.read_csv(transactions_agg_file)\n",
        "        print(f\"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(visits_df)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {transactions_agg_file}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"–§–∞–π–ª {transactions_agg_file} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    required_columns = ['clientID', 'visit_date', 'item', 'itemGroup', 'quantity', 'amount']\n",
        "    missing_columns = [col for col in required_columns if col not in visits_df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {missing_columns}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—É –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ datetime\n",
        "    observation_end_date = pd.to_datetime(observation_end_date)\n",
        "\n",
        "    # 3. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º visit_date –≤ datetime\n",
        "    visits_df = visits_df.copy()\n",
        "    visits_df['visit_date'] = pd.to_datetime(visits_df['visit_date'])\n",
        "\n",
        "    # 4. –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∏–∑–∏—Ç—ã –°–¢–†–û–ì–û –î–û observation_end_date (–≤–∞–∂–Ω–æ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö)\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –°–¢–†–û–ì–û–ï –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ: visit_date < observation_end_date\n",
        "    filtered_visits = visits_df[visits_df['visit_date'] < observation_end_date].copy()\n",
        "\n",
        "    if filtered_visits.empty:\n",
        "        print(f\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã {observation_end_date.date()}\")\n",
        "        print(f\"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –≤ –¥–∞–Ω–Ω—ã—Ö: –æ—Ç {visits_df['visit_date'].min().date()} –¥–æ {visits_df['visit_date'].max().date()}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_visits)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    print(f\"–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –æ—Ç {filtered_visits['visit_date'].min().date()} \"\n",
        "          f\"–¥–æ {filtered_visits['visit_date'].max().date()} (–Ω–µ –≤–∫–ª—é—á–∞—è {observation_end_date.date()})\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "    unique_clients_before = visits_df['clientID'].nunique()\n",
        "    unique_clients_after = filtered_visits['clientID'].nunique()\n",
        "    print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: –±—ã–ª–æ {unique_clients_before}, —Å—Ç–∞–ª–æ {unique_clients_after}\")\n",
        "\n",
        "    # 5. –°–æ–∑–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame –ø–æ –≤–∏–∑–∏—Ç–∞–º (clientID + visit_date)\n",
        "    visits_agg = filtered_visits.groupby(['clientID', 'visit_date']).agg({\n",
        "        'quantity': 'sum',      # —Å—É–º–º–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –≤–∏–∑–∏—Ç–µ\n",
        "        'amount': 'sum',        # —Å—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫ –≤ –≤–∏–∑–∏—Ç–µ\n",
        "        'item': 'nunique'       # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –≤–∏–∑–∏—Ç–µ\n",
        "    }).reset_index()\n",
        "\n",
        "    # 6. –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –¥–Ω—è (—Å—É–±–±–æ—Ç–∞=5, –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ=6)\n",
        "    visits_agg['is_weekend'] = visits_agg['visit_date'].dt.dayofweek.isin([5, 6])\n",
        "\n",
        "    # 7. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "\n",
        "    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–ª—è RFM –∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    client_stats = visits_agg.groupby('clientID').agg({\n",
        "        'visit_date': ['max', 'nunique'],          # –ø–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç (Frequency)\n",
        "        'amount': 'sum',                           # –æ–±—â–∞—è —Å—É–º–º–∞ (Monetary)\n",
        "        'quantity': 'sum',                         # –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤\n",
        "        'is_weekend': 'sum'                        # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ\n",
        "    })\n",
        "\n",
        "    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º—É–ª—å—Ç–∏–∏–Ω–¥–µ–∫—Å\n",
        "    client_stats.columns = [\n",
        "        'last_visit_date', 'Frequency',\n",
        "        'Monetary', 'total_quantity', 'weekend_visits'\n",
        "    ]\n",
        "\n",
        "    # 8. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Recency (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–Ω–∏–º –≤–∏–∑–∏—Ç–æ–º –∏ observation_end_date)\n",
        "    client_stats['Recency'] = (observation_end_date - client_stats['last_visit_date']).dt.days\n",
        "\n",
        "    # 9. –ü–æ–ª—É—á–∞–µ–º —Å—É–º–º—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –∏ –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞\n",
        "    last_visits = visits_agg.sort_values('visit_date').groupby('clientID').last()\n",
        "    client_stats['amount_last_visit'] = last_visits['amount']\n",
        "\n",
        "    # 10. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –∑–∞ –∏—Å—Ç–æ—Ä–∏—é –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    unique_items_stats = filtered_visits.groupby('clientID').agg({\n",
        "        'item': 'nunique'\n",
        "    }).rename(columns={'item': 'total_unique_items'})\n",
        "\n",
        "    # 11. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "    profiles_df = client_stats.merge(unique_items_stats, left_index=True, right_index=True)\n",
        "\n",
        "    # 12. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    # –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: –æ–±—â–∞—è —Å—É–º–º–∞ / –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑–∏—Ç–æ–≤\n",
        "    profiles_df['avg_check'] = profiles_df['Monetary'] / profiles_df['Frequency']\n",
        "\n",
        "    # –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –≤–∏–∑–∏—Ç\n",
        "    profiles_df['avg_items_per_visit'] = profiles_df['total_quantity'] / profiles_df['Frequency']\n",
        "\n",
        "    # 13. –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ Frequency = 0, —Ö–æ—Ç—è —ç—Ç–æ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ)\n",
        "    profiles_df['avg_check'] = profiles_df['avg_check'].fillna(0)\n",
        "    profiles_df['avg_items_per_visit'] = profiles_df['avg_items_per_visit'].fillna(0)\n",
        "\n",
        "    # 14. –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º\n",
        "    profiles_df = profiles_df.reset_index().rename(columns={'index': 'clientID'})\n",
        "\n",
        "    # 15. –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã\n",
        "    column_order = [\n",
        "        'clientID', 'Recency', 'Frequency', 'Monetary',\n",
        "        'last_visit_date', 'total_quantity', 'avg_check',\n",
        "        'total_unique_items', 'avg_items_per_visit',\n",
        "        'weekend_visits', 'amount_last_visit'\n",
        "    ]\n",
        "\n",
        "    profiles_df = profiles_df[column_order]\n",
        "\n",
        "    # 16. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ clientID\n",
        "    profiles_df = profiles_df.sort_values('clientID').reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n–†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–ª—è {len(profiles_df)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "    print(f\"–î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ): {observation_end_date.date()}\")\n",
        "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π:\")\n",
        "    print(f\"- Recency: {profiles_df['Recency'].min()}-{profiles_df['Recency'].max()} –¥–Ω–µ–π \"\n",
        "          f\"(—Å—Ä–µ–¥–Ω–µ–µ: {profiles_df['Recency'].mean():.1f})\")\n",
        "    print(f\"- Frequency: {profiles_df['Frequency'].min()}-{profiles_df['Frequency'].max()} –≤–∏–∑–∏—Ç–æ–≤ \"\n",
        "          f\"(—Å—Ä–µ–¥–Ω–µ–µ: {profiles_df['Frequency'].mean():.1f})\")\n",
        "    print(f\"- Monetary: {profiles_df['Monetary'].min():.2f}-{profiles_df['Monetary'].max():.2f} —Ä—É–±. \"\n",
        "          f\"(—Å—Ä–µ–¥–Ω–µ–µ: {profiles_df['Monetary'].mean():.2f})\")\n",
        "\n",
        "    return profiles_df\n",
        "\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "def save_client_profiles(profiles_df, output_file='client_profiles_until_2019-10-01.csv'):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ CSV —Ñ–∞–π–ª.\n",
        "    \"\"\"\n",
        "    if not profiles_df.empty:\n",
        "        profiles_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n–ü—Ä–æ—Ñ–∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file}\")\n",
        "\n",
        "        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ\n",
        "        print(f\"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(profiles_df)} —Å—Ç—Ä–æ–∫ √ó {len(profiles_df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ 2019-10-01\n",
        "if __name__ == \"__main__\":\n",
        "    # –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–∏–∑–∏—Ç–∞–º–∏\n",
        "    transactions_agg_file = 'transactions_aggregated.csv'\n",
        "\n",
        "    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –î–û 2019-10-01 (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–†–ê–°–ß–ï–¢ –ü–†–û–§–ò–õ–ï–ô –ö–õ–ò–ï–ù–¢–û–í –î–û 2019-10-01\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client_profiles = calculate_client_profile_at_date(\n",
        "        transactions_agg_file,\n",
        "        observation_end_date='2019-10-01'  # –°—Ç—Ä–æ–≥–æ –¥–æ —ç—Ç–æ–π –¥–∞—Ç—ã\n",
        "    )\n",
        "\n",
        "    # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
        "    if not client_profiles.empty:\n",
        "        print(\"\\n–ü–µ—Ä–≤—ã–µ 10 –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "        print(client_profiles.head(10))\n",
        "\n",
        "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Recency\n",
        "        print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Recency (–¥–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞):\")\n",
        "        recency_bins = pd.cut(client_profiles['Recency'],\n",
        "                             bins=[0, 7, 30, 90, 180, 365, 1000],\n",
        "                             labels=['0-7', '8-30', '31-90', '91-180', '181-365', '>365'])\n",
        "        recency_dist = recency_bins.value_counts().sort_index()\n",
        "        print(recency_dist)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "        save_client_profiles(client_profiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "_e2CbAsJYCld",
        "outputId": "32e9eeb3-942b-4052-e87b-e2a29c0eb1f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "–†–ê–°–ß–ï–¢ –ü–†–û–§–ò–õ–ï–ô –ö–õ–ò–ï–ù–¢–û–í –î–û 2019-10-01\n",
            "============================================================\n",
            "–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ 1003083 –∑–∞–ø–∏—Å–µ–π –∏–∑ transactions_aggregated.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1355452178.py:48: UserWarning:\n",
            "\n",
            "Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å 947269 –∑–∞–ø–∏—Å–µ–π\n",
            "–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –æ—Ç 2017-09-01 –¥–æ 2019-09-30 (–Ω–µ –≤–∫–ª—é—á–∞—è 2019-10-01)\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: –±—ã–ª–æ 42746, —Å—Ç–∞–ª–æ 41196\n",
            "\n",
            "–†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–ª—è 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–î–∞—Ç–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–Ω–µ –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ): 2019-10-01\n",
            "\n",
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π:\n",
            "- Recency: 1-760 –¥–Ω–µ–π (—Å—Ä–µ–¥–Ω–µ–µ: 198.4)\n",
            "- Frequency: 1-168 –≤–∏–∑–∏—Ç–æ–≤ (—Å—Ä–µ–¥–Ω–µ–µ: 6.0)\n",
            "- Monetary: 10.00-3887309.00 —Ä—É–±. (—Å—Ä–µ–¥–Ω–µ–µ: 14431.69)\n",
            "\n",
            "–ü–µ—Ä–≤—ã–µ 10 –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "      clientID  Recency  Frequency  Monetary last_visit_date  total_quantity  \\\n",
            "0      client1      617          1      2273      2018-01-22               9   \n",
            "1     client10       57          1      4757      2019-08-05               3   \n",
            "2    client100      146          1      7299      2019-05-08               1   \n",
            "3   client1000       17         13     36495      2019-09-14             166   \n",
            "4  client10000      426          1      8495      2018-08-01               5   \n",
            "5  client10001       51          5      1874      2019-08-11              33   \n",
            "6  client10002      124          4      5850      2019-05-30              25   \n",
            "7  client10003       19         41     57061      2019-09-12             586   \n",
            "8  client10004        2         12      9495      2019-09-29              61   \n",
            "9  client10005      146          5      4846      2019-05-08              33   \n",
            "\n",
            "     avg_check  total_unique_items  avg_items_per_visit  weekend_visits  \\\n",
            "0  2273.000000                   4             9.000000               0   \n",
            "1  4757.000000                   3             3.000000               0   \n",
            "2  7299.000000                   1             1.000000               0   \n",
            "3  2807.307692                  31            12.769231               5   \n",
            "4  8495.000000                   5             5.000000               0   \n",
            "5   374.800000                  18             6.600000               4   \n",
            "6  1462.500000                  18             6.250000               2   \n",
            "7  1391.731707                 110            14.292683               5   \n",
            "8   791.250000                  41             5.083333               6   \n",
            "9   969.200000                  22             6.600000               0   \n",
            "\n",
            "   amount_last_visit  \n",
            "0               2273  \n",
            "1               4757  \n",
            "2               7299  \n",
            "3               4703  \n",
            "4               8495  \n",
            "5                179  \n",
            "6                839  \n",
            "7               1174  \n",
            "8               1733  \n",
            "9                171  \n",
            "\n",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Recency (–¥–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞):\n",
            "Recency\n",
            "0-7        3022\n",
            "8-30       5799\n",
            "31-90      8501\n",
            "91-180     7727\n",
            "181-365    7068\n",
            ">365       9079\n",
            "Name: count, dtype: int64\n",
            "\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: client_profiles_until_2019-10-01.csv\n",
            "–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: 41196 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π (–¢–ï–°–¢–û–í–ê–Ø –≤—ã–±–æ—Ä–∫–∞)**\n",
        "\n",
        "def mark_events(visits_df, result_start_date, result_end_date)"
      ],
      "metadata": {
        "id": "H8mLQjNkb90o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "\n",
        "def mark_events(visits_df, result_start_date, result_end_date):\n",
        "    \"\"\"\n",
        "    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –ø–æ—Å–µ—â–∞–ª –ª–∏ –∫–ª–∏–µ–Ω—Ç –º–∞–≥–∞–∑–∏–Ω –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ [result_start, result_end).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    visits_df : pandas.DataFrame\n",
        "        DataFrame —Å –≤–∏–∑–∏—Ç–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤. –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã:\n",
        "        'clientID' (–∏–ª–∏ 'client') –∏ 'visit_date'\n",
        "    result_start_date : str –∏–ª–∏ datetime-like\n",
        "        –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞ (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ).\n",
        "    result_end_date : str –∏–ª–∏ datetime-like\n",
        "        –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞ (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame —Å –¥–≤—É–º—è –∫–æ–ª–æ–Ω–∫–∞–º–∏:\n",
        "        - client: –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã\n",
        "        - event: True –µ—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –ø–æ—Å–µ—Ç–∏–ª –º–∞–≥–∞–∑–∏–Ω –≤ –ø–µ—Ä–∏–æ–¥, –∏–Ω–∞—á–µ False\n",
        "    \"\"\"\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–∞ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏\n",
        "    client_column = None\n",
        "    possible_client_columns = ['clientID', 'client', 'customer_id', 'customerID']\n",
        "\n",
        "    for col in possible_client_columns:\n",
        "        if col in visits_df.columns:\n",
        "            client_column = col\n",
        "            break\n",
        "\n",
        "    if client_column is None:\n",
        "        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∏–º–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–æ—Ö–æ–∂–∏–π —Å—Ç–æ–ª–±–µ—Ü\n",
        "        for col in visits_df.columns:\n",
        "            if 'client' in col.lower() or 'customer' in col.lower():\n",
        "                client_column = col\n",
        "                break\n",
        "\n",
        "    if client_column is None:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤. \"\n",
        "                        \"–û–∂–∏–¥–∞–µ–º—ã–µ –∏–º–µ–Ω–∞: 'clientID', 'client', 'customer_id', 'customerID'\")\n",
        "\n",
        "    if 'visit_date' not in visits_df.columns:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'visit_date'\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ datetime\n",
        "    result_start_date = pd.to_datetime(result_start_date)\n",
        "    result_end_date = pd.to_datetime(result_end_date)\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º visit_date –≤ datetime, –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω\n",
        "    visits_df = visits_df.copy()\n",
        "    if not pd.api.types.is_datetime64_any_dtype(visits_df['visit_date']):\n",
        "        visits_df['visit_date'] = pd.to_datetime(visits_df['visit_date'])\n",
        "\n",
        "    # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    all_clients = visits_df[client_column].unique()\n",
        "    print(f\"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {len(all_clients)}\")\n",
        "\n",
        "    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∏–∑–∏—Ç—ã –≤ –°–¢–†–û–ì–û–ú –¥–∏–∞–ø–∞–∑–æ–Ω–µ:\n",
        "    # visit_date >= result_start_date –ò visit_date < result_end_date\n",
        "    mask = (visits_df['visit_date'] >= result_start_date) & (visits_df['visit_date'] < result_end_date)\n",
        "    period_visits = visits_df[mask].copy()\n",
        "\n",
        "    print(f\"–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å {result_start_date.date()} –ø–æ {result_end_date.date()} (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\")\n",
        "    print(f\"–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: {len(period_visits)}\")\n",
        "    print(f\"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: {period_visits[client_column].nunique()}\")\n",
        "\n",
        "    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –∫–ª–∏–µ–Ω—Ç—ã –∏–º–µ–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –≤–∏–∑–∏—Ç –≤ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥\n",
        "    clients_with_events = period_visits[client_column].unique()\n",
        "\n",
        "    # 4. –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
        "    result_df = pd.DataFrame({\n",
        "        'client': all_clients,\n",
        "        'event': pd.Series(all_clients).isin(clients_with_events).values\n",
        "    })\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    event_counts = result_df['event'].value_counts()\n",
        "    if True in event_counts.index:\n",
        "        true_count = event_counts[True]\n",
        "    else:\n",
        "        true_count = 0\n",
        "\n",
        "    print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\")\n",
        "    print(f\"- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: {true_count} ({true_count/len(result_df)*100:.1f}%)\")\n",
        "    print(f\"- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: {len(result_df) - true_count} ({(len(result_df)-true_count)/len(result_df)*100:.1f}%)\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "def mark_events_fast(visits_df, result_start_date, result_end_date):\n",
        "    \"\"\"\n",
        "    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ mark_events.\n",
        "    \"\"\"\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' not in visits_df.columns and 'client' not in visits_df.columns:\n",
        "        raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "\n",
        "    client_column = 'clientID' if 'clientID' in visits_df.columns else 'client'\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã\n",
        "    result_start_date = pd.to_datetime(result_start_date)\n",
        "    result_end_date = pd.to_datetime(result_end_date)\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º visit_date\n",
        "    visits_df = visits_df.copy()\n",
        "    visits_df['visit_date'] = pd.to_datetime(visits_df['visit_date'])\n",
        "\n",
        "    # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    all_clients = pd.Series(visits_df[client_column].unique(), name='client')\n",
        "\n",
        "    # 2. –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∏–∑–∏—Ç—ã –≤ –ø–µ—Ä–∏–æ–¥–µ\n",
        "    period_mask = (visits_df['visit_date'] >= result_start_date) & (visits_df['visit_date'] < result_end_date)\n",
        "\n",
        "    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Å–æ–±—ã—Ç–∏—è–º–∏\n",
        "    clients_with_events = visits_df.loc[period_mask, client_column].unique()\n",
        "\n",
        "    # 4. –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–º–æ—â—å—é –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
        "    result_df = pd.DataFrame({\n",
        "        'client': all_clients,\n",
        "        'event': all_clients.isin(clients_with_events)\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏\n",
        "if __name__ == \"__main__\":\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    test_data = pd.DataFrame({\n",
        "        'clientID': ['client1', 'client1', 'client2', 'client3', 'client2', 'client4', 'client1'],\n",
        "        'visit_date': ['2019-09-01', '2019-09-15', '2019-09-05',\n",
        "                      '2019-10-10', '2019-10-20', '2019-11-05', '2019-12-01'],\n",
        "        'item': ['item1', 'item2', 'item1', 'item3', 'item2', 'item1', 'item2'],\n",
        "        'quantity': [1, 2, 3, 1, 2, 1, 1],\n",
        "        'amount': [100, 200, 150, 300, 120, 80, 90]\n",
        "    })\n",
        "\n",
        "    print(\"–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:\")\n",
        "    print(test_data)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # –¢–µ—Å—Ç 1: –ü–µ—Ä–∏–æ–¥, –≤ –∫–æ—Ç–æ—Ä–æ–º –µ—Å—Ç—å –≤–∏–∑–∏—Ç—ã\n",
        "    print(\"–¢–µ—Å—Ç 1: –ü–µ—Ä–∏–æ–¥ —Å 2019-09-01 –ø–æ 2019-10-01\")\n",
        "    result1 = mark_events(test_data, '2019-09-01', '2019-10-01')\n",
        "    print(result1)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # –¢–µ—Å—Ç 2: –ü–µ—Ä–∏–æ–¥, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    print(\"–¢–µ—Å—Ç 2: –ü–µ—Ä–∏–æ–¥ —Å 2019-10-01 –ø–æ 2019-11-01\")\n",
        "    result2 = mark_events(test_data, '2019-10-01', '2019-11-01')\n",
        "    print(result2)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # –¢–µ—Å—Ç 3: –ü—É—Å—Ç–æ–π –ø–µ—Ä–∏–æ–¥ (–Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤)\n",
        "    print(\"–¢–µ—Å—Ç 3: –ü–µ—Ä–∏–æ–¥ —Å 2020-01-01 –ø–æ 2020-02-01 (–Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤)\")\n",
        "    result3 = mark_events(test_data, '2020-01-01', '2020-02-01')\n",
        "    print(result3)\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    print(\"–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ñ–∞–π–ª–∞ transactions_aggregated.csv:\")\n",
        "\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
        "        visits_df = pd.read_csv('transactions_aggregated.csv')\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Å–µ–Ω—Ç—è–±—Ä—å 2019)\n",
        "        result_start = '2019-10-01'\n",
        "        result_end = '2019-11-01'  # –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ\n",
        "\n",
        "        result = mark_events(visits_df, result_start, result_end)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "        result.to_csv('client_events_october_2019.csv', index=False)\n",
        "        print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ client_events_october_2019.csv\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"–§–∞–π–ª transactions_aggregated.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "dU4jabquZGf7",
        "outputId": "ba64bec9-4443-43eb-becc-a9d184a3812a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:\n",
            "  clientID  visit_date   item  quantity  amount\n",
            "0  client1  2019-09-01  item1         1     100\n",
            "1  client1  2019-09-15  item2         2     200\n",
            "2  client2  2019-09-05  item1         3     150\n",
            "3  client3  2019-10-10  item3         1     300\n",
            "4  client2  2019-10-20  item2         2     120\n",
            "5  client4  2019-11-05  item1         1      80\n",
            "6  client1  2019-12-01  item2         1      90\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ—Å—Ç 1: –ü–µ—Ä–∏–æ–¥ —Å 2019-09-01 –ø–æ 2019-10-01\n",
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 4\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2019-09-01 –ø–æ 2019-10-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 3\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 2\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 2 (50.0%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 2 (50.0%)\n",
            "    client  event\n",
            "0  client1   True\n",
            "1  client2   True\n",
            "2  client3  False\n",
            "3  client4  False\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ—Å—Ç 2: –ü–µ—Ä–∏–æ–¥ —Å 2019-10-01 –ø–æ 2019-11-01\n",
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 4\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2019-10-01 –ø–æ 2019-11-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 2\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 2\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 2 (50.0%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 2 (50.0%)\n",
            "    client  event\n",
            "0  client1  False\n",
            "1  client2   True\n",
            "2  client3   True\n",
            "3  client4  False\n",
            "\n",
            "==================================================\n",
            "\n",
            "–¢–µ—Å—Ç 3: –ü–µ—Ä–∏–æ–¥ —Å 2020-01-01 –ø–æ 2020-02-01 (–Ω–µ—Ç –≤–∏–∑–∏—Ç–æ–≤)\n",
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 4\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2020-01-01 –ø–æ 2020-02-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 0\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 0\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 0 (0.0%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 4 (100.0%)\n",
            "    client  event\n",
            "0  client1  False\n",
            "1  client2  False\n",
            "2  client3  False\n",
            "3  client4  False\n",
            "\n",
            "==================================================\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ñ–∞–π–ª–∞ transactions_aggregated.csv:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-92309470.py:57: UserWarning:\n",
            "\n",
            "Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: 42746\n",
            "–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: —Å 2019-10-01 –ø–æ 2019-11-01 (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)\n",
            "–ù–∞–π–¥–µ–Ω–æ –≤–∏–∑–∏—Ç–æ–≤ –≤ –ø–µ—Ä–∏–æ–¥: 55814\n",
            "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –≤–∏–∑–∏—Ç–∞–º–∏ –≤ –ø–µ—Ä–∏–æ–¥: 9324\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏:\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=True: 9324 (21.8%)\n",
            "- –ö–ª–∏–µ–Ω—Ç–æ–≤ —Å event=False: 33422 (78.2%)\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ client_events_october_2019.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ —Å–æ–±—ã—Ç–∏–π (–¢–ï–°–¢–û–í–ê–Ø –≤—ã–±–æ—Ä–∫–∞)**\n",
        "\n",
        "def create_training_sample(profile_df, events_df)"
      ],
      "metadata": {
        "id": "iBh86VIjcK8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_training_sample(profile_df, events_df):\n",
        "    \"\"\"\n",
        "    –°–æ–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏—è –≤ –µ–¥–∏–Ω—É—é –≤—ã–±–æ—Ä–∫—É.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    profile_df : pandas.DataFrame\n",
        "        DataFrame —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç calculate_client_profile_at_date).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'.\n",
        "    events_df : pandas.DataFrame\n",
        "        DataFrame —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏–π (—Ä–µ–∑—É–ª—å—Ç–∞—Ç mark_events).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã 'client' –∏ 'event'.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã\n",
        "    profile = profile_df.copy()\n",
        "    events = events_df.copy()\n",
        "\n",
        "    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' not in profile.columns:\n",
        "        raise ValueError(\"profile_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'\")\n",
        "\n",
        "    if 'client' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'client'\")\n",
        "\n",
        "    if 'event' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'event'\")\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü –≤ profile –¥–ª—è join (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' in profile.columns and 'client' in events.columns:\n",
        "        print(f\"–ü—Ä–æ—Ñ–∏–ª–∏: {len(profile)} –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: {len(events)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: {list(profile.columns)}\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: {list(events.columns)}\")\n",
        "\n",
        "        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –¥–ª—è join\n",
        "        profile = profile.rename(columns={'clientID': 'client'})\n",
        "\n",
        "    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø—Ä–æ—Ñ–∏–ª—è –∏ —Å–æ–±—ã—Ç–∏–π –ø–æ 'client'\n",
        "    print(f\"\\n1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\")\n",
        "    before_join = len(profile)\n",
        "\n",
        "    training_sample = pd.merge(\n",
        "        profile,\n",
        "        events[['client', 'event']],  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n",
        "        on='client',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    after_join = len(training_sample)\n",
        "    print(f\"   –î–æ join: {before_join} –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\")\n",
        "    print(f\"   –ü–æ—Å–ª–µ inner join: {after_join} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "    print(f\"   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {before_join - after_join} (–Ω–µ—Ç –≤ events_df)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–∞ –ª–∏ –≤—ã–±–æ—Ä–∫–∞\n",
        "    if training_sample.empty:\n",
        "        raise ValueError(\"–†–µ–∑—É–ª—å—Ç–∞—Ç inner join –ø—É—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤.\")\n",
        "\n",
        "    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'event' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "    print(f\"\\n2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\")\n",
        "    missing_events = training_sample['event'].isnull().sum()\n",
        "\n",
        "    if missing_events > 0:\n",
        "        print(f\"   –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {missing_events} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π!\")\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–µ–º False\")\n",
        "        training_sample['event'] = training_sample['event'].fillna(False)\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    if training_sample['event'].dtype != 'bool':\n",
        "        print(f\"   –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 'event' –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø\")\n",
        "        training_sample['event'] = training_sample['event'].astype(bool)\n",
        "\n",
        "    # 4. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏ (–æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
        "    print(f\"\\n3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\")\n",
        "\n",
        "    date_columns = []\n",
        "    for col in training_sample.columns:\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–æ–ª–±–µ—Ü datetime –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 'date' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏\n",
        "        if (pd.api.types.is_datetime64_any_dtype(training_sample[col]) or\n",
        "            'date' in col.lower() or\n",
        "            col.lower() in ['last_visit_date', 'visit_date']):\n",
        "            date_columns.append(col)\n",
        "\n",
        "    if date_columns:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: {date_columns}\")\n",
        "        training_sample = training_sample.drop(columns=date_columns)\n",
        "    else:\n",
        "        print(f\"   –°—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "\n",
        "    # 5. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –≤—ã–±–æ—Ä–∫–µ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Å–æ–±—ã—Ç–∏—è (True –∏ False)\n",
        "    print(f\"\\n4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\")\n",
        "\n",
        "    class_counts = training_sample['event'].value_counts()\n",
        "    print(f\"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    for value, count in class_counts.items():\n",
        "        percentage = count / len(training_sample) * 100\n",
        "        print(f\"   - event={value}: {count} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤\n",
        "    unique_classes = training_sample['event'].nunique()\n",
        "    if unique_classes < 2:\n",
        "        raise ValueError(f\"–í –≤—ã–±–æ—Ä–∫–µ —Ç–æ–ª—å–∫–æ {unique_classes} –∫–ª–∞—Å—Å! –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –æ–±–∞ –∫–ª–∞—Å—Å–∞ (True –∏ False).\")\n",
        "\n",
        "    # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "    print(f\"\\n5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\")\n",
        "    missing_values = training_sample.isnull().sum()\n",
        "    columns_with_missing = missing_values[missing_values > 0]\n",
        "\n",
        "    if len(columns_with_missing) > 0:\n",
        "        print(f\"   –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö:\")\n",
        "        for col, count in columns_with_missing.items():\n",
        "            percentage = count / len(training_sample) * 100\n",
        "            print(f\"   - {col}: {count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏...\")\n",
        "        for col in columns_with_missing.index:\n",
        "            if pd.api.types.is_numeric_dtype(training_sample[col]):\n",
        "                median_val = training_sample[col].median()\n",
        "                training_sample[col] = training_sample[col].fillna(median_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–µ–¥–∏–∞–Ω–æ–π ({median_val:.2f})\")\n",
        "            else:\n",
        "                # –î–ª—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º —Å–∞–º—ã–º —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "                mode_val = training_sample[col].mode()[0] if not training_sample[col].mode().empty else 'Unknown'\n",
        "                training_sample[col] = training_sample[col].fillna(mode_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–æ–¥–æ–π ({mode_val})\")\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\")\n",
        "\n",
        "    # 7. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "    print(f\"\\n6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä: {len(training_sample)} —Å—Ç—Ä–æ–∫ √ó {len(training_sample.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    print(f\"   –°—Ç–æ–ª–±—Ü—ã: {list(training_sample.columns)}\")\n",
        "\n",
        "    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    print(f\"   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    for col in training_sample.columns:\n",
        "        dtype = training_sample[col].dtype\n",
        "        if col == 'event':\n",
        "            print(f\"   - {col}: {dtype} (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\")\n",
        "        elif col == 'client':\n",
        "            print(f\"   - {col}: {dtype} (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\")\n",
        "        else:\n",
        "            print(f\"   - {col}: {dtype}\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
        "    numeric_cols = training_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'event' in numeric_cols:\n",
        "        numeric_cols.remove('event')\n",
        "\n",
        "    if numeric_cols:\n",
        "        print(f\"\\n   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\")\n",
        "        for col in numeric_cols[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏\n",
        "            if col in training_sample.columns:\n",
        "                print(f\"   - {col}: min={training_sample[col].min():.2f}, \"\n",
        "                      f\"max={training_sample[col].max():.2f}, \"\n",
        "                      f\"mean={training_sample[col].mean():.2f}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return training_sample\n",
        "\n",
        "\n",
        "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏\n",
        "def save_training_sample(training_sample, output_file='training_sample.csv'):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –≤ CSV —Ñ–∞–π–ª.\n",
        "    \"\"\"\n",
        "    if not training_sample.empty:\n",
        "        training_sample.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_file}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –≤–µ—Ä—Å–∏—é –±–µ–∑ client –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "        features_sample = training_sample.drop(columns=['client'] if 'client' in training_sample.columns else [])\n",
        "        features_file = 'training_sample_features1.csv'\n",
        "        features_sample.to_csv(features_file, index=False, encoding='utf-8')\n",
        "        print(f\"–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {features_file}\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    test_profiles = pd.DataFrame({\n",
        "        'clientID': ['client1', 'client2', 'client3', 'client4', 'client5'],\n",
        "        'Recency': [15, 45, 120, 7, 90],\n",
        "        'Frequency': [5, 2, 1, 10, 3],\n",
        "        'Monetary': [1500, 800, 300, 5000, 1200],\n",
        "        'last_visit_date': ['2019-08-15', '2019-07-15', '2019-05-01', '2019-08-24', '2019-06-01'],\n",
        "        'total_quantity': [25, 10, 3, 50, 15],\n",
        "        'avg_check': [300, 400, 300, 500, 400],\n",
        "        'total_unique_items': [8, 5, 2, 15, 6],\n",
        "        'avg_items_per_visit': [5, 5, 3, 5, 5],\n",
        "        'weekend_visits': [2, 1, 0, 4, 1],\n",
        "        'amount_last_visit': [350, 450, 300, 600, 420]\n",
        "    })\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π\n",
        "    test_events = pd.DataFrame({\n",
        "        'client': ['client1', 'client2', 'client3', 'client4', 'client6'],  # client5 –Ω–µ—Ç, client6 –Ω–µ—Ç –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
        "        'event': [True, False, True, True, False]\n",
        "    })\n",
        "\n",
        "    print(\"–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\")\n",
        "    print(\"–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "    print(test_profiles)\n",
        "    print(\"\\n–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\")\n",
        "    print(test_events)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "    try:\n",
        "        training_sample = create_training_sample(test_profiles, test_events)\n",
        "\n",
        "        print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\")\n",
        "        print(training_sample)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∫–∏: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç)\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–æ 2019-10-01)\n",
        "        profiles = pd.read_csv('client_profiles_until_2019-10-01.csv')\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ –æ–∫—Ç—è–±—Ä—å 2019)\n",
        "        events = pd.read_csv('client_events_october_2019.csv')\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "        real_training_sample = create_training_sample(profiles, events)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "        save_training_sample(real_training_sample, 'test_sample.csv')\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {e}\")\n",
        "        print(\"–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\")\n",
        "        print(\"1. calculate_client_profile_at_date() - –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π\")\n",
        "        print(\"2. mark_events() - –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "yY4flYMeaXWz",
        "outputId": "3aca8b52-0e31-4f99-ff43-1d1cc1a13ec3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "  clientID  Recency  Frequency  Monetary last_visit_date  total_quantity  \\\n",
            "0  client1       15          5      1500      2019-08-15              25   \n",
            "1  client2       45          2       800      2019-07-15              10   \n",
            "2  client3      120          1       300      2019-05-01               3   \n",
            "3  client4        7         10      5000      2019-08-24              50   \n",
            "4  client5       90          3      1200      2019-06-01              15   \n",
            "\n",
            "   avg_check  total_unique_items  avg_items_per_visit  weekend_visits  \\\n",
            "0        300                   8                    5               2   \n",
            "1        400                   5                    5               1   \n",
            "2        300                   2                    3               0   \n",
            "3        500                  15                    5               4   \n",
            "4        400                   6                    5               1   \n",
            "\n",
            "   amount_last_visit  \n",
            "0                350  \n",
            "1                450  \n",
            "2                300  \n",
            "3                600  \n",
            "4                420  \n",
            "\n",
            "–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\n",
            "    client  event\n",
            "0  client1   True\n",
            "1  client2  False\n",
            "2  client3   True\n",
            "3  client4   True\n",
            "4  client6  False\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 1 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=True: 3 –∫–ª–∏–µ–Ω—Ç–æ–≤ (75.0%)\n",
            "   - event=False: 1 –∫–ª–∏–µ–Ω—Ç–æ–≤ (25.0%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 4 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: int64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: int64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=7.00, max=120.00, mean=46.75\n",
            "   - Frequency: min=1.00, max=10.00, mean=4.50\n",
            "   - Monetary: min=300.00, max=5000.00, mean=1900.00\n",
            "   - total_quantity: min=3.00, max=50.00, mean=22.00\n",
            "   - avg_check: min=300.00, max=500.00, mean=375.00\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\n",
            "    client  Recency  Frequency  Monetary  total_quantity  avg_check  \\\n",
            "0  client1       15          5      1500              25        300   \n",
            "1  client2       45          2       800              10        400   \n",
            "2  client3      120          1       300               3        300   \n",
            "3  client4        7         10      5000              50        500   \n",
            "\n",
            "   total_unique_items  avg_items_per_visit  weekend_visits  amount_last_visit  \\\n",
            "0                   8                    5               2                350   \n",
            "1                   5                    5               1                450   \n",
            "2                   2                    3               0                300   \n",
            "3                  15                    5               4                600   \n",
            "\n",
            "   event  \n",
            "0   True  \n",
            "1  False  \n",
            "2   True  \n",
            "3   True  \n",
            "\n",
            "============================================================\n",
            "–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\n",
            "============================================================\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 42746 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 0 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=False: 33422 –∫–ª–∏–µ–Ω—Ç–æ–≤ (81.1%)\n",
            "   - event=True: 7774 –∫–ª–∏–µ–Ω—Ç–æ–≤ (18.9%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 41196 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: float64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: float64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=1.00, max=760.00, mean=198.35\n",
            "   - Frequency: min=1.00, max=168.00, mean=6.01\n",
            "   - Monetary: min=10.00, max=3887309.00, mean=14431.69\n",
            "   - total_quantity: min=1.00, max=5396.00, mean=51.31\n",
            "   - avg_check: min=10.00, max=125860.00, mean=2953.43\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: test_sample.csv\n",
            "–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: training_sample_features1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ClearML —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á–∏\n",
        "try:\n",
        "    %env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "    %env CLEARML_API_HOST=https://api.clear.ml\n",
        "    %env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "    %env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "    %env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "    from clearml import Task, Dataset\n",
        "\n",
        "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–¥–∞—á—É –≤ ClearML\n",
        "    task = Task.init(\n",
        "        project_name=\"CourseInz\",\n",
        "        task_name=\"test_sample_creation\",\n",
        "        tags=[\"test_sample\"]\n",
        "    )\n",
        "\n",
        "    clearml_available = True\n",
        "    print(\"‚úÖ ClearML —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  ClearML –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}\")\n",
        "    print(\"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ ClearML...\")\n",
        "    clearml_available = False\n",
        "    task = None\n",
        "\n",
        "def create_training_sample(profile_df, events_df):\n",
        "    \"\"\"\n",
        "    –°–æ–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏—è –≤ –µ–¥–∏–Ω—É—é –≤—ã–±–æ—Ä–∫—É.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    profile_df : pandas.DataFrame\n",
        "        DataFrame —Å –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç calculate_client_profile_at_date).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'.\n",
        "    events_df : pandas.DataFrame\n",
        "        DataFrame —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ–±—ã—Ç–∏–π (—Ä–µ–∑—É–ª—å—Ç–∞—Ç mark_events).\n",
        "        –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã 'client' –∏ 'event'.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –Ω–µ –º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã\n",
        "    profile = profile_df.copy()\n",
        "    events = events_df.copy()\n",
        "\n",
        "    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' not in profile.columns:\n",
        "        raise ValueError(\"profile_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'clientID'\")\n",
        "\n",
        "    if 'client' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'client'\")\n",
        "\n",
        "    if 'event' not in events.columns:\n",
        "        raise ValueError(\"events_df –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'event'\")\n",
        "\n",
        "    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü –≤ profile –¥–ª—è join (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ –∏–º–µ–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    if 'clientID' in profile.columns and 'client' in events.columns:\n",
        "        print(f\"–ü—Ä–æ—Ñ–∏–ª–∏: {len(profile)} –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: {len(events)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: {list(profile.columns)}\")\n",
        "        print(f\"–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: {list(events.columns)}\")\n",
        "\n",
        "        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –¥–ª—è join\n",
        "        profile = profile.rename(columns={'clientID': 'client'})\n",
        "\n",
        "    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø—Ä–æ—Ñ–∏–ª—è –∏ —Å–æ–±—ã—Ç–∏–π –ø–æ 'client'\n",
        "    print(f\"\\n1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\")\n",
        "    before_join = len(profile)\n",
        "\n",
        "    training_sample = pd.merge(\n",
        "        profile,\n",
        "        events[['client', 'event']],  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n",
        "        on='client',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    after_join = len(training_sample)\n",
        "    print(f\"   –î–æ join: {before_join} –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\")\n",
        "    print(f\"   –ü–æ—Å–ª–µ inner join: {after_join} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "    print(f\"   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {before_join - after_join} (–Ω–µ—Ç –≤ events_df)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–∞ –ª–∏ –≤—ã–±–æ—Ä–∫–∞\n",
        "    if training_sample.empty:\n",
        "        raise ValueError(\"–†–µ–∑—É–ª—å—Ç–∞—Ç inner join –ø—É—Å—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤.\")\n",
        "\n",
        "    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'event' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "    print(f\"\\n2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\")\n",
        "    missing_events = training_sample['event'].isnull().sum()\n",
        "\n",
        "    if missing_events > 0:\n",
        "        print(f\"   –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {missing_events} –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π!\")\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∑–Ω–∞—á–µ–Ω–∏–µ–º False\")\n",
        "        training_sample['event'] = training_sample['event'].fillna(False)\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\")\n",
        "\n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º event –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
        "    if training_sample['event'].dtype != 'bool':\n",
        "        print(f\"   –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º 'event' –≤ –±—É–ª–µ–≤—ã–π —Ç–∏–ø\")\n",
        "        training_sample['event'] = training_sample['event'].astype(bool)\n",
        "\n",
        "    # 4. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏ (–æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
        "    print(f\"\\n3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\")\n",
        "\n",
        "    date_columns = []\n",
        "    for col in training_sample.columns:\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–æ–ª–±–µ—Ü datetime –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç 'date' –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏\n",
        "        if (pd.api.types.is_datetime64_any_dtype(training_sample[col]) or\n",
        "            'date' in col.lower() or\n",
        "            col.lower() in ['last_visit_date', 'visit_date']):\n",
        "            date_columns.append(col)\n",
        "\n",
        "    if date_columns:\n",
        "        print(f\"   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: {date_columns}\")\n",
        "        training_sample = training_sample.drop(columns=date_columns)\n",
        "    else:\n",
        "        print(f\"   –°—Ç–æ–ª–±—Ü—ã —Å –¥–∞—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
        "\n",
        "    # 5. –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –≤—ã–±–æ—Ä–∫–µ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞ –¥–ª—è —Å–æ–±—ã—Ç–∏—è (True –∏ False)\n",
        "    print(f\"\\n4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\")\n",
        "\n",
        "    class_counts = training_sample['event'].value_counts()\n",
        "    print(f\"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    for value, count in class_counts.items():\n",
        "        percentage = count / len(training_sample) * 100\n",
        "        print(f\"   - event={value}: {count} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤\n",
        "    unique_classes = training_sample['event'].nunique()\n",
        "    if unique_classes < 2:\n",
        "        raise ValueError(f\"–í –≤—ã–±–æ—Ä–∫–µ —Ç–æ–ª—å–∫–æ {unique_classes} –∫–ª–∞—Å—Å! –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –æ–±–∞ –∫–ª–∞—Å—Å–∞ (True –∏ False).\")\n",
        "\n",
        "    # 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "    print(f\"\\n5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\")\n",
        "    missing_values = training_sample.isnull().sum()\n",
        "    columns_with_missing = missing_values[missing_values > 0]\n",
        "\n",
        "    if len(columns_with_missing) > 0:\n",
        "        print(f\"   –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Å—Ç–æ–ª–±—Ü–∞—Ö:\")\n",
        "        for col, count in columns_with_missing.items():\n",
        "            percentage = count / len(training_sample) * 100\n",
        "            print(f\"   - {col}: {count} –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({percentage:.1f}%)\")\n",
        "\n",
        "        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "        print(f\"   –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏...\")\n",
        "        for col in columns_with_missing.index:\n",
        "            if pd.api.types.is_numeric_dtype(training_sample[col]):\n",
        "                median_val = training_sample[col].median()\n",
        "                training_sample[col] = training_sample[col].fillna(median_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–µ–¥–∏–∞–Ω–æ–π ({median_val:.2f})\")\n",
        "            else:\n",
        "                # –î–ª—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö - –∑–∞–ø–æ–ª–Ω—è–µ–º —Å–∞–º—ã–º —á–∞—Å—Ç—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º\n",
        "                mode_val = training_sample[col].mode()[0] if not training_sample[col].mode().empty else 'Unknown'\n",
        "                training_sample[col] = training_sample[col].fillna(mode_val)\n",
        "                print(f\"     {col}: –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –º–æ–¥–æ–π ({mode_val})\")\n",
        "    else:\n",
        "        print(f\"   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\")\n",
        "\n",
        "    # 7. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "    print(f\"\\n6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä: {len(training_sample)} —Å—Ç—Ä–æ–∫ √ó {len(training_sample.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    print(f\"   –°—Ç–æ–ª–±—Ü—ã: {list(training_sample.columns)}\")\n",
        "\n",
        "    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "    print(f\"   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    for col in training_sample.columns:\n",
        "        dtype = training_sample[col].dtype\n",
        "        if col == 'event':\n",
        "            print(f\"   - {col}: {dtype} (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\")\n",
        "        elif col == 'client':\n",
        "            print(f\"   - {col}: {dtype} (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\")\n",
        "        else:\n",
        "            print(f\"   - {col}: {dtype}\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n",
        "    numeric_cols = training_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'event' in numeric_cols:\n",
        "        numeric_cols.remove('event')\n",
        "\n",
        "    if numeric_cols:\n",
        "        print(f\"\\n   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\")\n",
        "        for col in numeric_cols[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏\n",
        "            if col in training_sample.columns:\n",
        "                print(f\"   - {col}: min={training_sample[col].min():.2f}, \"\n",
        "                      f\"max={training_sample[col].max():.2f}, \"\n",
        "                      f\"mean={training_sample[col].mean():.2f}\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return training_sample\n",
        "\n",
        "\n",
        "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏\n",
        "def save_training_sample(training_sample, output_file='training_sample.csv'):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –≤ CSV —Ñ–∞–π–ª.\n",
        "    \"\"\"\n",
        "    if not training_sample.empty:\n",
        "        training_sample.to_csv(output_file, index=False, encoding='utf-8')\n",
        "        print(f\"\\n–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_file}\")\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –≤–µ—Ä—Å–∏—é –±–µ–∑ client –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "        features_sample = training_sample.drop(columns=['client'] if 'client' in training_sample.columns else [])\n",
        "        features_file = 'training_sample_features1.csv'\n",
        "        features_sample.to_csv(features_file, index=False, encoding='utf-8')\n",
        "        print(f\"–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {features_file}\")\n",
        "    else:\n",
        "        print(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –≤ ClearML\n",
        "def save_to_clearml(file_path, dataset_name=\"test_sample\"):\n",
        "    \"\"\"\n",
        "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª –≤ ClearML –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\n",
        "    \"\"\"\n",
        "    if not clearml_available:\n",
        "        print(\"‚ö†Ô∏è  ClearML –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"–°–û–•–†–ê–ù–ï–ù–ò–ï –í CLEARML\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
        "        return None, None\n",
        "\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(f\"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data)} —Å—Ç—Ä–æ–∫, {len(data.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏\n",
        "    print(\"\\n1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏...\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
        "    metadata = {\n",
        "        \"file_name\": file_path.name,\n",
        "        \"file_size_bytes\": os.path.getsize(file_path),\n",
        "        \"rows\": len(data),\n",
        "        \"columns\": list(data.columns),\n",
        "        \"creation_date\": datetime.datetime.now().isoformat(),\n",
        "        \"file_path\": str(file_path.absolute())\n",
        "    }\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–±—ã—Ç–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–∫–æ–π —Å—Ç–æ–ª–±–µ—Ü\n",
        "    if 'event' in data.columns:\n",
        "        event_true = int(data['event'].sum())\n",
        "        event_false = int((~data['event']).sum())\n",
        "        metadata[\"event_distribution\"] = {\n",
        "            \"True\": event_true,\n",
        "            \"False\": event_false,\n",
        "            \"true_percentage\": float(event_true/len(data)*100) if len(data) > 0 else 0,\n",
        "            \"false_percentage\": float(event_false/len(data)*100) if len(data) > 0 else 0\n",
        "        }\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç\n",
        "    task.upload_artifact(\n",
        "        name=file_path.stem,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è\n",
        "        artifact_object=str(file_path),\n",
        "        metadata=metadata\n",
        "    )\n",
        "\n",
        "    print(f\"   ‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç '{file_path.name}' –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
        "\n",
        "    # 2. –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    print(\"\\n2. –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...\")\n",
        "\n",
        "    # –ì–æ—Ç–æ–≤–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "    description = f\"\"\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤.\n",
        "–§–∞–π–ª: {file_path.name}\n",
        "–°–æ–∑–¥–∞–Ω–æ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "–†–∞–∑–º–µ—Ä: {len(data)} —Å—Ç—Ä–æ–∫ √ó {len(data.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤\n",
        "–°—Ç–æ–ª–±—Ü—ã: {', '.join(data.columns)}\n",
        "\"\"\"\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "    if 'event' in data.columns:\n",
        "        event_true = data['event'].sum()\n",
        "        event_false = (~data['event']).sum()\n",
        "        total = len(data)\n",
        "\n",
        "        description += f\"\"\"\n",
        "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
        "  - True: {event_true} ({event_true/total*100:.1f}%)\n",
        "  - False: {event_false} ({event_false/total*100:.1f}%)\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "        dataset = Dataset.create(\n",
        "            dataset_name=dataset_name,\n",
        "            dataset_project=\"CourseInz/Datasets\",\n",
        "            dataset_version=None,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ—Ä—Å–∏—è\n",
        "            description=description,\n",
        "            parent_datasets=None\n",
        "        )\n",
        "\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª\n",
        "        dataset.add_files(path=str(file_path))\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫\n",
        "        output_dir = Path(\"clearml_temp\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        columns_info = pd.DataFrame({\n",
        "            'column': data.columns,\n",
        "            'dtype': data.dtypes.astype(str),\n",
        "            'non_null': data.notnull().sum().values,\n",
        "            'null': data.isnull().sum().values,\n",
        "            'description': [\n",
        "                '–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞' if col == 'client' else\n",
        "                '–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –ø–æ—Å–µ—â–µ–Ω–∏–µ –≤ –ø–µ—Ä–∏–æ–¥' if col == 'event' else\n",
        "                f'–ü—Ä–∏–∑–Ω–∞–∫: {col}'\n",
        "                for col in data.columns\n",
        "            ]\n",
        "        })\n",
        "        columns_file = output_dir / \"columns_description.csv\"\n",
        "        columns_info.to_csv(columns_file, index=False)\n",
        "        dataset.add_files(path=str(columns_file))\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
        "        metadata_file = output_dir / \"dataset_metadata.json\"\n",
        "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "        dataset.add_files(path=str(metadata_file))\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML\n",
        "        print(\"   –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML...\")\n",
        "        dataset.upload()\n",
        "\n",
        "        # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
        "        dataset.finalize()\n",
        "\n",
        "        print(f\"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {dataset.name}, –≤–µ—Ä—Å–∏—è: {dataset.version}\")\n",
        "        print(f\"   üìä ID –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset.id}\")\n",
        "\n",
        "        # 3. –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤ –∑–∞–¥–∞—á–µ\n",
        "        task.set_parameter(\"dataset_id\", dataset.id)\n",
        "        task.set_parameter(\"dataset_name\", dataset.name)\n",
        "        task.set_parameter(\"dataset_version\", dataset.version)\n",
        "\n",
        "        # 4. –õ–æ–≥–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–∏–º–µ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö\n",
        "        logger = task.get_logger()\n",
        "        logger.report_table(\n",
        "            title=f\"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {file_path.name} (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫)\",\n",
        "            series=\"test_sample\",\n",
        "            table_plot=data.head(10),\n",
        "            iteration=0\n",
        "        )\n",
        "\n",
        "        # 5. –õ–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "        logger.report_single_value(\n",
        "            name=\"test_sample_rows\",\n",
        "            value=len(data)\n",
        "        )\n",
        "        logger.report_single_value(\n",
        "            name=\"test_sample_columns\",\n",
        "            value=len(data.columns)\n",
        "        )\n",
        "\n",
        "        if 'event' in data.columns:\n",
        "            event_true = data['event'].sum()\n",
        "            event_false = (~data['event']).sum()\n",
        "            total = len(data)\n",
        "\n",
        "            logger.report_single_value(\n",
        "                name=\"event_true_count\",\n",
        "                value=int(event_true)\n",
        "            )\n",
        "            logger.report_single_value(\n",
        "                name=\"event_false_count\",\n",
        "                value=int(event_false)\n",
        "            )\n",
        "\n",
        "            if total > 0:\n",
        "                logger.report_single_value(\n",
        "                    name=\"event_true_percentage\",\n",
        "                    value=float(event_true/total*100)\n",
        "                )\n",
        "                logger.report_single_value(\n",
        "                    name=\"event_false_percentage\",\n",
        "                    value=float(event_false/total*100)\n",
        "                )\n",
        "\n",
        "        print(\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ClearML:\")\n",
        "        print(f\"   üìÅ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç: {file_path.stem}\")\n",
        "        print(f\"   üóÇÔ∏è  –î–∞—Ç–∞—Å–µ—Ç: {dataset.name} v{dataset.version}\")\n",
        "        if task is not None:\n",
        "            print(f\"   üîó –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")\n",
        "\n",
        "        return dataset, file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ ClearML: {e}\")\n",
        "        return None, file_path\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "if __name__ == \"__main__\":\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
        "    test_profiles = pd.DataFrame({\n",
        "        'clientID': ['client1', 'client2', 'client3', 'client4', 'client5'],\n",
        "        'Recency': [15, 45, 120, 7, 90],\n",
        "        'Frequency': [5, 2, 1, 10, 3],\n",
        "        'Monetary': [1500, 800, 300, 5000, 1200],\n",
        "        'last_visit_date': ['2019-08-15', '2019-07-15', '2019-05-01', '2019-08-24', '2019-06-01'],\n",
        "        'total_quantity': [25, 10, 3, 50, 15],\n",
        "        'avg_check': [300, 400, 300, 500, 400],\n",
        "        'total_unique_items': [8, 5, 2, 15, 6],\n",
        "        'avg_items_per_visit': [5, 5, 3, 5, 5],\n",
        "        'weekend_visits': [2, 1, 0, 4, 1],\n",
        "        'amount_last_visit': [350, 450, 300, 600, 420]\n",
        "    })\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π\n",
        "    test_events = pd.DataFrame({\n",
        "        'client': ['client1', 'client2', 'client3', 'client4', 'client6'],  # client5 –Ω–µ—Ç, client6 –Ω–µ—Ç –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
        "        'event': [True, False, True, True, False]\n",
        "    })\n",
        "\n",
        "    print(\"–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\")\n",
        "    print(\"–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\")\n",
        "    print(test_profiles)\n",
        "    print(\"\\n–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\")\n",
        "    print(test_events)\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "    try:\n",
        "        training_sample = create_training_sample(test_profiles, test_events)\n",
        "\n",
        "        print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\")\n",
        "        print(training_sample)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∫–∏: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –ü—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ —Ñ–∞–π–ª—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç)\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ (—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–æ 2019-10-01)\n",
        "        profiles = pd.read_csv('client_profiles_until_2019-10-01.csv')\n",
        "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–æ—Ñ–∏–ª–µ–π: {len(profiles)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É —Å–æ–±—ã—Ç–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ –æ–∫—Ç—è–±—Ä—å 2019)\n",
        "        events = pd.read_csv('client_events_october_2019.csv')\n",
        "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(events)} –∫–ª–∏–µ–Ω—Ç–æ–≤\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "        real_training_sample = create_training_sample(profiles, events)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "        save_training_sample(real_training_sample, 'test_sample.csv')\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤ ClearML\n",
        "        dataset, saved_file = save_to_clearml('test_sample.csv', dataset_name=\"test_sample_october_2019\")\n",
        "\n",
        "        print(f\"\\n‚úÖ –§–∞–π–ª 'test_sample.csv' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ ClearML\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå –§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {e}\")\n",
        "        print(\"–î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\")\n",
        "        print(\"1. calculate_client_profile_at_date() - –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π\")\n",
        "        print(\"2. mark_events() - –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∫–∏: {e}\")\n",
        "\n",
        "# –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É ClearML –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞\n",
        "if clearml_available and task is not None:\n",
        "    task.close()\n",
        "    print(\"\\n‚úÖ –ó–∞–¥–∞—á–∞ ClearML –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "eYJKXhs6QqFu",
        "outputId": "d8c3a915-1853-4291-c134-24bb8f1778de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "ClearML Task: created new task id=aeff8a57ce2d4892a020718f8b09ab80\n",
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/aeff8a57ce2d4892a020718f8b09ab80/output/log\n",
            "‚úÖ ClearML —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω\n",
            "–¢–ï–°–¢–û–í–´–ô –ü–†–ò–ú–ï–†\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:\n",
            "  clientID  Recency  Frequency  Monetary last_visit_date  total_quantity  \\\n",
            "0  client1       15          5      1500      2019-08-15              25   \n",
            "1  client2       45          2       800      2019-07-15              10   \n",
            "2  client3      120          1       300      2019-05-01               3   \n",
            "3  client4        7         10      5000      2019-08-24              50   \n",
            "4  client5       90          3      1200      2019-06-01              15   \n",
            "\n",
            "   avg_check  total_unique_items  avg_items_per_visit  weekend_visits  \\\n",
            "0        300                   8                    5               2   \n",
            "1        400                   5                    5               1   \n",
            "2        300                   2                    3               0   \n",
            "3        500                  15                    5               4   \n",
            "4        400                   6                    5               1   \n",
            "\n",
            "   amount_last_visit  \n",
            "0                350  \n",
            "1                450  \n",
            "2                300  \n",
            "3                600  \n",
            "4                420  \n",
            "\n",
            "–†–∞–∑–º–µ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π:\n",
            "    client  event\n",
            "0  client1   True\n",
            "1  client2  False\n",
            "2  client3   True\n",
            "3  client4   True\n",
            "4  client6  False\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 5 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 4 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 1 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=True: 3 –∫–ª–∏–µ–Ω—Ç–æ–≤ (75.0%)\n",
            "   - event=False: 1 –∫–ª–∏–µ–Ω—Ç–æ–≤ (25.0%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 4 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: int64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: int64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=7.00, max=120.00, mean=46.75\n",
            "   - Frequency: min=1.00, max=10.00, mean=4.50\n",
            "   - Monetary: min=300.00, max=5000.00, mean=1900.00\n",
            "   - total_quantity: min=3.00, max=50.00, mean=22.00\n",
            "   - avg_check: min=300.00, max=500.00, mean=375.00\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:\n",
            "    client  Recency  Frequency  Monetary  total_quantity  avg_check  \\\n",
            "0  client1       15          5      1500              25        300   \n",
            "1  client2       45          2       800              10        400   \n",
            "2  client3      120          1       300               3        300   \n",
            "3  client4        7         10      5000              50        500   \n",
            "\n",
            "   total_unique_items  avg_items_per_visit  weekend_visits  amount_last_visit  \\\n",
            "0                   8                    5               2                350   \n",
            "1                   5                    5               1                450   \n",
            "2                   2                    3               0                300   \n",
            "3                  15                    5               4                600   \n",
            "\n",
            "   event  \n",
            "0   True  \n",
            "1  False  \n",
            "2   True  \n",
            "3   True  \n",
            "\n",
            "============================================================\n",
            "–ü–†–ò–ú–ï–† –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò\n",
            "============================================================\n",
            "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–æ—Ñ–∏–ª–µ–π: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: 42746 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "============================================================\n",
            "–°–û–ó–î–ê–ù–ò–ï –û–ë–£–ß–ê–Æ–©–ï–ô –í–´–ë–û–†–ö–ò\n",
            "============================================================\n",
            "–ü—Ä–æ—Ñ–∏–ª–∏: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤, —Å–æ–±—ã—Ç–∏—è: 42746 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "–°—Ç–æ–ª–±—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è: ['clientID', 'Recency', 'Frequency', 'Monetary', 'last_visit_date', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit']\n",
            "–°—Ç–æ–ª–±—Ü—ã —Å–æ–±—ã—Ç–∏–π: ['client', 'event']\n",
            "\n",
            "1. –í—ã–ø–æ–ª–Ω—è–µ–º inner join –ø–æ 'client'...\n",
            "   –î–æ join: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª—è—Ö\n",
            "   –ü–æ—Å–ª–µ inner join: 41196 –∫–ª–∏–µ–Ω—Ç–æ–≤\n",
            "   –£–¥–∞–ª–µ–Ω–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: 0 (–Ω–µ—Ç –≤ events_df)\n",
            "\n",
            "2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'event'...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Ç\n",
            "\n",
            "3. –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏...\n",
            "   –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã: ['last_visit_date']\n",
            "\n",
            "4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤...\n",
            "   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n",
            "   - event=False: 33422 –∫–ª–∏–µ–Ω—Ç–æ–≤ (81.1%)\n",
            "   - event=True: 7774 –∫–ª–∏–µ–Ω—Ç–æ–≤ (18.9%)\n",
            "\n",
            "5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö...\n",
            "   –ü—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –Ω–µ—Ç\n",
            "\n",
            "6. –§–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞:\n",
            "   –†–∞–∑–º–µ—Ä: 41196 —Å—Ç—Ä–æ–∫ √ó 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "   –°—Ç–æ–ª–±—Ü—ã: ['client', 'Recency', 'Frequency', 'Monetary', 'total_quantity', 'avg_check', 'total_unique_items', 'avg_items_per_visit', 'weekend_visits', 'amount_last_visit', 'event']\n",
            "   –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n",
            "   - client: object (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
            "   - Recency: int64\n",
            "   - Frequency: int64\n",
            "   - Monetary: int64\n",
            "   - total_quantity: int64\n",
            "   - avg_check: float64\n",
            "   - total_unique_items: int64\n",
            "   - avg_items_per_visit: float64\n",
            "   - weekend_visits: int64\n",
            "   - amount_last_visit: int64\n",
            "   - event: bool (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
            "\n",
            "   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:\n",
            "   - Recency: min=1.00, max=760.00, mean=198.35\n",
            "   - Frequency: min=1.00, max=168.00, mean=6.01\n",
            "   - Monetary: min=10.00, max=3887309.00, mean=14431.69\n",
            "   - total_quantity: min=1.00, max=5396.00, mean=51.31\n",
            "   - avg_check: min=10.00, max=125860.00, mean=2953.43\n",
            "============================================================\n",
            "–í–´–ë–û–†–ö–ê –ì–û–¢–û–í–ê –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø\n",
            "============================================================\n",
            "\n",
            "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: test_sample.csv\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "–í–µ—Ä—Å–∏—è –±–µ–∑ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: training_sample_features1.csv\n",
            "\n",
            "============================================================\n",
            "–°–û–•–†–ê–ù–ï–ù–ò–ï –í CLEARML\n",
            "============================================================\n",
            "‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: 41196 —Å—Ç—Ä–æ–∫, 11 —Å—Ç–æ–ª–±—Ü–æ–≤\n",
            "\n",
            "1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –∑–∞–¥–∞—á–∏...\n",
            "   ‚úÖ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç 'test_sample.csv' –∑–∞–≥—Ä—É–∂–µ–Ω\n",
            "\n",
            "2. –°–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...\n",
            "ClearML results page: https://app.clear.ml/projects/5d7e795557814b7385ce195cbffba714/experiments/848dba735ae44c4c9fb3e6004483c4e1/output/log\n",
            "ClearML dataset page: https://app.clear.ml/datasets/simple/5d7e795557814b7385ce195cbffba714/experiments/848dba735ae44c4c9fb3e6004483c4e1\n",
            "   –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å–µ—Ä–≤–µ—Ä ClearML...\n",
            "Uploading dataset changes (3 files compressed to 759.88 KiB) to https://files.clear.ml\n",
            "File compression and upload completed: total size 759.88 KiB, 1 chunk(s) stored (average size 759.88 KiB)\n",
            "   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: test_sample_october_2019, –≤–µ—Ä—Å–∏—è: 1.0.0\n",
            "   üìä ID –¥–∞—Ç–∞—Å–µ—Ç–∞: 848dba735ae44c4c9fb3e6004483c4e1\n",
            "\n",
            "‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ClearML:\n",
            "   üìÅ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç: test_sample\n",
            "   üóÇÔ∏è  –î–∞—Ç–∞—Å–µ—Ç: test_sample_october_2019 v1.0.0\n",
            "   üîó –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/aeff8a57ce2d4892a020718f8b09ab80\n",
            "\n",
            "‚úÖ –§–∞–π–ª 'test_sample.csv' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ ClearML\n",
            "\n",
            "‚úÖ –ó–∞–¥–∞—á–∞ ClearML –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´\n"
      ],
      "metadata": {
        "id": "37vEHcpfca9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: Random Forest\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                           f1_score, roc_auc_score, confusion_matrix)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from clearml import Task, Dataset\n",
        "\n",
        "# ============================================\n",
        "# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ ClearML\n",
        "# ============================================\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"RandomForest_FullFeatures\",\n",
        "    task_type=Task.TaskTypes.training,\n",
        "    tags=[\"experiment1\", \"random_forest\", \"full_features\"]\n",
        ")\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ\n",
        "print(\"=\" * 60)\n",
        "print(\"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: Random Forest —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ID –∑–∞–¥–∞—á–∏: {task.id}\")\n",
        "print(f\"–°—Å—ã–ª–∫–∞: https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")\n",
        "\n",
        "# ============================================\n",
        "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Dataset\n",
        "# ============================================\n",
        "print(\"\\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Dataset...\")\n",
        "\n",
        "# –£–∫–∞–∂–∏—Ç–µ ID –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–æ–π\n",
        "# (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π ID –∏–∑ –≤–∞—à–µ–≥–æ ClearML)\n",
        "dataset_train_id = \"da192194265543feaaf7c1dd5045e35e\"  # –í–∞—à dataset ID\n",
        "\n",
        "try:\n",
        "    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
        "    dataset_train = Dataset.get(dataset_id=dataset_train_id)\n",
        "\n",
        "    # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "    dataset_path = dataset_train.get_local_copy()\n",
        "    print(f\"–î–∞—Ç–∞—Å–µ—Ç —Å–∫–∞—á–∞–Ω –≤: {dataset_path}\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É\n",
        "    train_data_path = f\"{dataset_path}/train_sample.csv\"\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    print(f\"‚úÖ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {train_df.shape}\")\n",
        "\n",
        "    # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "    task.set_parameter(\"dataset_id\", dataset_train.id)\n",
        "    task.set_parameter(\"dataset_name\", dataset_train.name)\n",
        "    task.set_parameter(\"dataset_version\", dataset_train.version)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}\")\n",
        "    print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª...\")\n",
        "    train_df = pd.read_csv(\"train_sample.csv\")\n",
        "    print(f\"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {train_df.shape}\")\n",
        "\n",
        "# ============================================\n",
        "# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "# ============================================\n",
        "print(\"\\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "X = train_df.drop(['client', 'event'], axis=1)\n",
        "y = train_df['event'].astype(int)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º bool –≤ int\n",
        "\n",
        "print(f\"–ü—Ä–∏–∑–Ω–∞–∫–∏: {X.shape}\")\n",
        "print(f\"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {y.shape}\")\n",
        "print(f\"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"  Train: {X_train.shape}\")\n",
        "print(f\"  Validation: {X_val.shape}\")\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# ============================================\n",
        "# 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "# ============================================\n",
        "print(\"\\nüìä –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\")\n",
        "\n",
        "hyperparams = {\n",
        "    'model_type': 'RandomForestClassifier',\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 10,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 2,\n",
        "    'max_features': 'sqrt',\n",
        "    'random_state': 42,\n",
        "    'test_size': 0.2,\n",
        "    'stratify': True,\n",
        "    'scaler': 'StandardScaler'\n",
        "}\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ ClearML\n",
        "for key, value in hyperparams.items():\n",
        "    task.set_parameter(key, value)\n",
        "\n",
        "print(\"–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\")\n",
        "for key, value in hyperparams.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# ============================================\n",
        "# 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "# ============================================\n",
        "print(\"\\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest...\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=hyperparams['n_estimators'],\n",
        "    max_depth=hyperparams['max_depth'],\n",
        "    min_samples_split=hyperparams['min_samples_split'],\n",
        "    min_samples_leaf=hyperparams['min_samples_leaf'],\n",
        "    max_features=hyperparams['max_features'],\n",
        "    random_state=hyperparams['random_state'],\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\")\n",
        "\n",
        "# ============================================\n",
        "# 6. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
        "# ============================================\n",
        "print(\"\\nüìà –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞...\")\n",
        "\n",
        "# –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "y_pred = model.predict(X_val_scaled)\n",
        "y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "# –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
        "metrics = {\n",
        "    'accuracy': accuracy_score(y_val, y_pred),\n",
        "    'precision': precision_score(y_val, y_pred),\n",
        "    'recall': recall_score(y_val, y_pred),\n",
        "    'f1': f1_score(y_val, y_pred),\n",
        "    'roc_auc': roc_auc_score(y_val, y_pred_proba)\n",
        "}\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "metrics.update({\n",
        "    'true_negative': int(tn),\n",
        "    'false_positive': int(fp),\n",
        "    'false_negative': int(fn),\n",
        "    'true_positive': int(tp)\n",
        "})\n",
        "\n",
        "# –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫\n",
        "print(\"\\nüìä –ú–ï–¢–†–ò–ö–ò –ù–ê –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–û–ô –í–´–ë–û–†–ö–ï:\")\n",
        "print(\"-\" * 40)\n",
        "for metric_name, metric_value in metrics.items():\n",
        "    if metric_name not in ['true_negative', 'false_positive', 'false_negative', 'true_positive']:\n",
        "        print(f\"{metric_name.upper():<15}: {metric_value:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä CONFUSION MATRIX:\")\n",
        "print(f\"True Negative:  {tn}\")\n",
        "print(f\"False Positive: {fp}\")\n",
        "print(f\"False Negative: {fn}\")\n",
        "print(f\"True Positive:  {tp}\")\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ ClearML\n",
        "logger = task.get_logger()\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "for metric_name, metric_value in metrics.items():\n",
        "    logger.report_single_value(\n",
        "        name=f\"val_{metric_name}\",\n",
        "        value=metric_value\n",
        "    )\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º confusion matrix –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É\n",
        "conf_matrix_df = pd.DataFrame(\n",
        "    conf_matrix,\n",
        "    columns=['Predicted 0', 'Predicted 1'],\n",
        "    index=['Actual 0', 'Actual 1']\n",
        ")\n",
        "\n",
        "logger.report_table(\n",
        "    title=\"Confusion Matrix\",\n",
        "    series=\"validation\",\n",
        "    table_plot=conf_matrix_df,\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 7. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "# ============================================\n",
        "print(\"\\nüîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\n–¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "logger.report_table(\n",
        "    title=\"Feature Importance\",\n",
        "    series=\"features\",\n",
        "    table_plot=feature_importance,\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç\n",
        "# ============================================\n",
        "print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –º–æ–¥–µ–ª—å—é –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π\n",
        "model_artifact = {\n",
        "    'model': model,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': X.columns.tolist(),\n",
        "    'hyperparameters': hyperparams,\n",
        "    'metrics': metrics\n",
        "}\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ pickle —Ñ–∞–π–ª\n",
        "model_filename = \"random_forest_model_v1.pkl\"\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(model_artifact, f)\n",
        "\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {model_filename}\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –≤ ClearML\n",
        "task.upload_artifact(\n",
        "    name=\"random_forest_model\",\n",
        "    artifact_object=model_filename,\n",
        "    metadata={\n",
        "        'model_type': 'RandomForestClassifier',\n",
        "        'training_samples': len(X_train),\n",
        "        'validation_samples': len(X_val),\n",
        "        'features_count': len(X.columns),\n",
        "        'creation_date': pd.Timestamp.now().isoformat(),\n",
        "        'metrics': metrics\n",
        "    }\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 9. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "# ============================================\n",
        "print(\"\\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\")\n",
        "\n",
        "try:\n",
        "    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    dataset_test_id = \"848dba735ae44c4c9fb3e6004483c4e1\"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π ID\n",
        "    dataset_test = Dataset.get(dataset_id=dataset_test_id)\n",
        "    test_dataset_path = dataset_test.get_local_copy()\n",
        "    test_data_path = f\"{test_dataset_path}/test_sample.csv\"\n",
        "    test_df = pd.read_csv(test_data_path)\n",
        "\n",
        "    print(f\"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {test_df.shape}\")\n",
        "\n",
        "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    X_test = test_df.drop(['client', 'event'], axis=1)\n",
        "    y_test = test_df['event'].astype(int)\n",
        "\n",
        "    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "    y_test_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
        "    test_metrics = {\n",
        "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'precision': precision_score(y_test, y_test_pred),\n",
        "        'recall': recall_score(y_test, y_test_pred),\n",
        "        'f1': f1_score(y_test, y_test_pred),\n",
        "        'roc_auc': roc_auc_score(y_test, y_test_pred_proba)\n",
        "    }\n",
        "\n",
        "    print(\"\\nüìä –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:\")\n",
        "    print(\"-\" * 40)\n",
        "    for metric_name, metric_value in test_metrics.items():\n",
        "        print(f\"{metric_name.upper():<15}: {metric_value:.4f}\")\n",
        "        logger.report_single_value(\n",
        "            name=f\"test_{metric_name}\",\n",
        "            value=metric_value\n",
        "        )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {e}\")\n",
        "\n",
        "# ============================================\n",
        "# 10. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1 –ó–ê–í–ï–†–®–ï–ù!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
        "print(f\"  –ú–æ–¥–µ–ª—å: RandomForestClassifier\")\n",
        "print(f\"  –¢–æ—á–Ω–æ—Å—Ç—å (accuracy): {metrics['accuracy']:.4f}\")\n",
        "print(f\"  F1-–º–µ—Ä–∞: {metrics['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
        "print(f\"  –û–±—É—á–µ–Ω–æ –Ω–∞: {len(X_train)} –æ–±—Ä–∞–∑—Ü–∞—Ö\")\n",
        "print(f\"  –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –Ω–∞: {len(X_val)} –æ–±—Ä–∞–∑—Ü–∞—Ö\")\n",
        "\n",
        "print(f\"\\nüîó –°—Å—ã–ª–∫–∞ –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\")\n",
        "print(f\"  https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")\n",
        "\n",
        "# –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É\n",
        "task.close()\n",
        "print(\"‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–∫—Ä—ã—Ç–∞!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "zEjkQjMOchAD",
        "outputId": "05ae6197-ddb3-44e6-f9ef-737422c802f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "ClearML Task: created new task id=e27302b9b6a34f1c997f6dda27ee50b8\n",
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/e27302b9b6a34f1c997f6dda27ee50b8/output/log\n",
            "============================================================\n",
            "–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: Random Forest —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
            "============================================================\n",
            "ID –∑–∞–¥–∞—á–∏: e27302b9b6a34f1c997f6dda27ee50b8\n",
            "–°—Å—ã–ª–∫–∞: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/e27302b9b6a34f1c997f6dda27ee50b8\n",
            "\n",
            "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Dataset...\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "–î–∞—Ç–∞—Å–µ—Ç —Å–∫–∞—á–∞–Ω –≤: /root/.clearml/cache/storage_manager/datasets/ds_da192194265543feaaf7c1dd5045e35e\n",
            "‚úÖ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: (28701, 11)\n",
            "\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
            "–ü—Ä–∏–∑–Ω–∞–∫–∏: (28701, 9)\n",
            "–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: (28701,)\n",
            "–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {0: 23271, 1: 5430}\n",
            "\n",
            "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
            "  Train: (22960, 9)\n",
            "  Validation: (5741, 9)\n",
            "\n",
            "üìä –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\n",
            "–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
            "  model_type: RandomForestClassifier\n",
            "  n_estimators: 100\n",
            "  max_depth: 10\n",
            "  min_samples_split: 5\n",
            "  min_samples_leaf: 2\n",
            "  max_features: sqrt\n",
            "  random_state: 42\n",
            "  test_size: 0.2\n",
            "  stratify: True\n",
            "  scaler: StandardScaler\n",
            "\n",
            "üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    3.3s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\n",
            "\n",
            "üìà –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞...\n",
            "\n",
            "üìä –ú–ï–¢–†–ò–ö–ò –ù–ê –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–û–ô –í–´–ë–û–†–ö–ï:\n",
            "----------------------------------------\n",
            "ACCURACY       : 0.8274\n",
            "PRECISION      : 0.5971\n",
            "RECALL         : 0.2689\n",
            "F1             : 0.3708\n",
            "ROC_AUC        : 0.8026\n",
            "\n",
            "üìä CONFUSION MATRIX:\n",
            "True Negative:  4458\n",
            "False Positive: 197\n",
            "False Negative: 794\n",
            "True Positive:  292\n",
            "\n",
            "üîç –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\n",
            "\n",
            "–¢–æ–ø-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
            "               feature  importance\n",
            "0              Recency    0.347576\n",
            "1            Frequency    0.190013\n",
            "3       total_quantity    0.076686\n",
            "2             Monetary    0.069589\n",
            "8    amount_last_visit    0.066261\n",
            "5   total_unique_items    0.065168\n",
            "7       weekend_visits    0.063941\n",
            "4            avg_check    0.062090\n",
            "6  avg_items_per_visit    0.058677\n",
            "\n",
            "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
            "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: random_forest_model_v1.pkl\n",
            "\n",
            "üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% | 6.43/6.43 MB [00:00<00:00, 27.37MB/s]: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: (41196, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä –ú–ï–¢–†–ò–ö–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï:\n",
            "----------------------------------------\n",
            "ACCURACY       : 0.8321\n",
            "PRECISION      : 0.6181\n",
            "RECALL         : 0.2889\n",
            "F1             : 0.3938\n",
            "ROC_AUC        : 0.8051\n",
            "\n",
            "============================================================\n",
            "–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1 –ó–ê–í–ï–†–®–ï–ù!\n",
            "============================================================\n",
            "\n",
            "üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
            "  –ú–æ–¥–µ–ª—å: RandomForestClassifier\n",
            "  –¢–æ—á–Ω–æ—Å—Ç—å (accuracy): 0.8274\n",
            "  F1-–º–µ—Ä–∞: 0.3708\n",
            "  ROC-AUC: 0.8026\n",
            "  –û–±—É—á–µ–Ω–æ –Ω–∞: 22960 –æ–±—Ä–∞–∑—Ü–∞—Ö\n",
            "  –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –Ω–∞: 5741 –æ–±—Ä–∞–∑—Ü–∞—Ö\n",
            "\n",
            "üîó –°—Å—ã–ª–∫–∞ –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\n",
            "  https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/e27302b9b6a34f1c997f6dda27ee50b8\n",
            "‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–∫—Ä—ã—Ç–∞!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 2: Logistic Regression\n",
        "\n",
        "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
        "%env CLEARML_API_HOST=https://api.clear.ml\n",
        "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
        "%env CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
        "%env CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                           f1_score, roc_auc_score, confusion_matrix)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from clearml import Task, Dataset\n",
        "\n",
        "# ============================================\n",
        "# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ ClearML\n",
        "# ============================================\n",
        "task = Task.init(\n",
        "    project_name=\"CourseInz\",\n",
        "    task_name=\"LogisticRegression_RFM\",\n",
        "    task_type=Task.TaskTypes.training,\n",
        "    tags=[\"experiment2\", \"logistic_regression\", \"rfm_features\"]\n",
        ")\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ\n",
        "print(\"=\" * 60)\n",
        "print(\"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: Logistic Regression —Å RFM-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ID –∑–∞–¥–∞—á–∏: {task.id}\")\n",
        "print(f\"–°—Å—ã–ª–∫–∞: https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")\n",
        "\n",
        "# ============================================\n",
        "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Dataset\n",
        "# ============================================\n",
        "print(\"\\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Dataset...\")\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –¥–∞—Ç–∞—Å–µ—Ç, —á—Ç–æ –∏ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ 1\n",
        "dataset_train_id = \"da192194265543feaaf7c1dd5045e35e\"  # –í–∞—à dataset ID\n",
        "\n",
        "try:\n",
        "    dataset_train = Dataset.get(dataset_id=dataset_train_id)\n",
        "    dataset_path = dataset_train.get_local_copy()\n",
        "    print(f\"–î–∞—Ç–∞—Å–µ—Ç —Å–∫–∞—á–∞–Ω –≤: {dataset_path}\")\n",
        "\n",
        "    train_data_path = f\"{dataset_path}/train_sample.csv\"\n",
        "    train_df = pd.read_csv(train_data_path)\n",
        "    print(f\"‚úÖ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {train_df.shape}\")\n",
        "\n",
        "    task.set_parameter(\"dataset_id\", dataset_train.id)\n",
        "    task.set_parameter(\"dataset_name\", dataset_train.name)\n",
        "    task.set_parameter(\"dataset_version\", dataset_train.version)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}\")\n",
        "    train_df = pd.read_csv(\"train_sample.csv\")\n",
        "    print(f\"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {train_df.shape}\")\n",
        "\n",
        "# ============================================\n",
        "# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ RFM –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
        "# ============================================\n",
        "print(\"\\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ RFM –ø—Ä–∏–∑–Ω–∞–∫–∏)...\")\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ RFM –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö\n",
        "selected_features = [\n",
        "    'Recency', 'Frequency', 'Monetary',  # –û—Å–Ω–æ–≤–Ω—ã–µ RFM\n",
        "    'avg_check', 'total_unique_items',    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞–∂–Ω—ã–µ\n",
        "    'weekend_visits'                      # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π –ø—Ä–∏–∑–Ω–∞–∫\n",
        "]\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö\n",
        "available_features = [col for col in selected_features if col in train_df.columns]\n",
        "print(f\"–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {available_features}\")\n",
        "\n",
        "X = train_df[available_features].copy()\n",
        "y = train_df['event'].astype(int)\n",
        "\n",
        "print(f\"–ü—Ä–∏–∑–Ω–∞–∫–∏: {X.shape}\")\n",
        "print(f\"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {y.shape}\")\n",
        "print(f\"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=123, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"  Train: {X_train.shape}\")\n",
        "print(f\"  Validation: {X_val.shape}\")\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# ============================================\n",
        "# 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "# ============================================\n",
        "print(\"\\nüìä –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\")\n",
        "\n",
        "hyperparams = {\n",
        "    'model_type': 'LogisticRegression',\n",
        "    'features': ', '.join(available_features),\n",
        "    'C': 0.1,  # –û–±—Ä–∞—Ç–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
        "    'penalty': 'l2',\n",
        "    'solver': 'lbfgs',\n",
        "    'max_iter': 1000,\n",
        "    'random_state': 123,\n",
        "    'test_size': 0.25,\n",
        "    'stratify': True,\n",
        "    'scaler': 'StandardScaler'\n",
        "}\n",
        "\n",
        "for key, value in hyperparams.items():\n",
        "    task.set_parameter(key, value)\n",
        "\n",
        "print(\"–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\")\n",
        "for key, value in hyperparams.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\n–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(available_features)}):\")\n",
        "for i, feature in enumerate(available_features, 1):\n",
        "    print(f\"  {i}. {feature}\")\n",
        "\n",
        "# ============================================\n",
        "# 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "# ============================================\n",
        "print(\"\\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Logistic Regression...\")\n",
        "\n",
        "model = LogisticRegression(\n",
        "    C=hyperparams['C'],\n",
        "    penalty=hyperparams['penalty'],\n",
        "    solver=hyperparams['solver'],\n",
        "    max_iter=hyperparams['max_iter'],\n",
        "    random_state=hyperparams['random_state']\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: {model.n_iter_[0]}\")\n",
        "\n",
        "# ============================================\n",
        "# 6. –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
        "# ============================================\n",
        "print(\"\\nüìà –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞...\")\n",
        "\n",
        "y_pred = model.predict(X_val_scaled)\n",
        "y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
        "\n",
        "metrics = {\n",
        "    'accuracy': accuracy_score(y_val, y_pred),\n",
        "    'precision': precision_score(y_val, y_pred),\n",
        "    'recall': recall_score(y_val, y_pred),\n",
        "    'f1': f1_score(y_val, y_pred),\n",
        "    'roc_auc': roc_auc_score(y_val, y_pred_proba)\n",
        "}\n",
        "\n",
        "conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "metrics.update({\n",
        "    'true_negative': int(tn),\n",
        "    'false_positive': int(fp),\n",
        "    'false_negative': int(fn),\n",
        "    'true_positive': int(tp)\n",
        "})\n",
        "\n",
        "print(\"\\nüìä –ú–ï–¢–†–ò–ö–ò –ù–ê –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–û–ô –í–´–ë–û–†–ö–ï:\")\n",
        "print(\"-\" * 40)\n",
        "for metric_name, metric_value in metrics.items():\n",
        "    if metric_name not in ['true_negative', 'false_positive', 'false_negative', 'true_positive']:\n",
        "        print(f\"{metric_name.upper():<15}: {metric_value:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä CONFUSION MATRIX:\")\n",
        "print(f\"True Negative:  {tn}\")\n",
        "print(f\"False Positive: {fp}\")\n",
        "print(f\"False Negative: {fn}\")\n",
        "print(f\"True Positive:  {tp}\")\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ ClearML\n",
        "logger = task.get_logger()\n",
        "\n",
        "for metric_name, metric_value in metrics.items():\n",
        "    logger.report_single_value(\n",
        "        name=f\"val_{metric_name}\",\n",
        "        value=metric_value\n",
        "    )\n",
        "\n",
        "conf_matrix_df = pd.DataFrame(\n",
        "    conf_matrix,\n",
        "    columns=['Predicted 0', 'Predicted 1'],\n",
        "    index=['Actual 0', 'Actual 1']\n",
        ")\n",
        "\n",
        "logger.report_table(\n",
        "    title=\"Confusion Matrix\",\n",
        "    series=\"validation\",\n",
        "    table_plot=conf_matrix_df,\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 7. –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏ (–≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
        "# ============================================\n",
        "print(\"\\nüîç –ê–Ω–∞–ª–∏–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏...\")\n",
        "\n",
        "coefficients = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'coefficient': model.coef_[0],\n",
        "    'abs_coefficient': np.abs(model.coef_[0])\n",
        "}).sort_values('abs_coefficient', ascending=False)\n",
        "\n",
        "print(\"\\n–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏:\")\n",
        "print(coefficients)\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã\n",
        "logger.report_table(\n",
        "    title=\"Model Coefficients\",\n",
        "    series=\"features\",\n",
        "    table_plot=coefficients,\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç\n",
        "# ============================================\n",
        "print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "\n",
        "model_artifact = {\n",
        "    'model': model,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': available_features,\n",
        "    'hyperparameters': hyperparams,\n",
        "    'metrics': metrics,\n",
        "    'coefficients': coefficients.to_dict()\n",
        "}\n",
        "\n",
        "model_filename = \"logistic_regression_model_v1.pkl\"\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(model_artifact, f)\n",
        "\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {model_filename}\")\n",
        "\n",
        "task.upload_artifact(\n",
        "    name=\"logistic_regression_model\",\n",
        "    artifact_object=model_filename,\n",
        "    metadata={\n",
        "        'model_type': 'LogisticRegression',\n",
        "        'features_count': len(available_features),\n",
        "        'training_samples': len(X_train),\n",
        "        'validation_samples': len(X_val),\n",
        "        'creation_date': pd.Timestamp.now().isoformat(),\n",
        "        'metrics': metrics\n",
        "    }\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 9. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "# ============================================\n",
        "print(\"\\nüìä –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...\")\n",
        "\n",
        "# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏\n",
        "thresholds = np.arange(0.1, 0.9, 0.1)\n",
        "threshold_results = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_val, y_pred_thresh)\n",
        "    precision = precision_score(y_val, y_pred_thresh)\n",
        "    recall = recall_score(y_val, y_pred_thresh)\n",
        "    f1 = f1_score(y_val, y_pred_thresh)\n",
        "\n",
        "    threshold_results.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_results)\n",
        "print(\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–∞—Ö:\")\n",
        "print(threshold_df)\n",
        "\n",
        "# –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Ä–æ–≥–æ–≤\n",
        "logger.report_table(\n",
        "    title=\"Threshold Analysis\",\n",
        "    series=\"thresholds\",\n",
        "    table_plot=threshold_df,\n",
        "    iteration=0\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 10. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2 –ó–ê–í–ï–†–®–ï–ù!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
        "print(f\"  –ú–æ–¥–µ–ª—å: LogisticRegression\")\n",
        "print(f\"  –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(available_features)}\")\n",
        "print(f\"  –¢–æ—á–Ω–æ—Å—Ç—å (accuracy): {metrics['accuracy']:.4f}\")\n",
        "print(f\"  F1-–º–µ—Ä–∞: {metrics['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
        "print(f\"  –°–∞–º—ã–π –≤–∞–∂–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫: {coefficients.iloc[0]['feature']}\")\n",
        "print(f\"  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {coefficients.iloc[0]['coefficient']:.4f}\")\n",
        "\n",
        "print(f\"\\nüîó –°—Å—ã–ª–∫–∞ –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\")\n",
        "print(f\"  https://app.clear.ml/projects/{task.project}/experiments/{task.id}\")\n",
        "\n",
        "# –ó–∞–∫—Ä—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É\n",
        "task.close()\n",
        "print(\"‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–∫—Ä—ã—Ç–∞!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw425VU0gs8n",
        "outputId": "2078b16e-e78e-4ee2-f389-c69abd8baebd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
            "env: CLEARML_API_HOST=https://api.clear.ml\n",
            "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
            "env: CLEARML_API_ACCESS_KEY=YNUIGW6OYGSLBCBWNDXN7X1HW0D8AY\n",
            "env: CLEARML_API_SECRET_KEY=La9XMoGmgU_B9qN-TQWj8wFLEYM28VmRA4oLBjs3iBfsyy1MbWIzYHGR9BlnTdRUuMk\n",
            "ClearML Task: created new task id=55111472edb24b66ade1b84ce77680c1\n",
            "ClearML results page: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/55111472edb24b66ade1b84ce77680c1/output/log\n",
            "============================================================\n",
            "–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2: Logistic Regression —Å RFM-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
            "============================================================\n",
            "ID –∑–∞–¥–∞—á–∏: 55111472edb24b66ade1b84ce77680c1\n",
            "–°—Å—ã–ª–∫–∞: https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/55111472edb24b66ade1b84ce77680c1\n",
            "\n",
            "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Dataset...\n",
            "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
            "–î–∞—Ç–∞—Å–µ—Ç —Å–∫–∞—á–∞–Ω –≤: /root/.clearml/cache/storage_manager/datasets/ds_da192194265543feaaf7c1dd5045e35e\n",
            "‚úÖ –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: (28701, 11)\n",
            "\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ RFM –ø—Ä–∏–∑–Ω–∞–∫–∏)...\n",
            "–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['Recency', 'Frequency', 'Monetary', 'avg_check', 'total_unique_items', 'weekend_visits']\n",
            "–ü—Ä–∏–∑–Ω–∞–∫–∏: (28701, 6)\n",
            "–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: (28701,)\n",
            "–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {0: 23271, 1: 5430}\n",
            "\n",
            "–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
            "  Train: (21525, 6)\n",
            "  Validation: (7176, 6)\n",
            "\n",
            "üìä –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...\n",
            "–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
            "  model_type: LogisticRegression\n",
            "  features: Recency, Frequency, Monetary, avg_check, total_unique_items, weekend_visits\n",
            "  C: 0.1\n",
            "  penalty: l2\n",
            "  solver: lbfgs\n",
            "  max_iter: 1000\n",
            "  random_state: 123\n",
            "  test_size: 0.25\n",
            "  stratify: True\n",
            "  scaler: StandardScaler\n",
            "\n",
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (6):\n",
            "  1. Recency\n",
            "  2. Frequency\n",
            "  3. Monetary\n",
            "  4. avg_check\n",
            "  5. total_unique_items\n",
            "  6. weekend_visits\n",
            "\n",
            "üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Logistic Regression...\n",
            "‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\n",
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: 13\n",
            "\n",
            "üìà –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞...\n",
            "\n",
            "üìä –ú–ï–¢–†–ò–ö–ò –ù–ê –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–û–ô –í–´–ë–û–†–ö–ï:\n",
            "----------------------------------------\n",
            "ACCURACY       : 0.8246\n",
            "PRECISION      : 0.6364\n",
            "RECALL         : 0.1701\n",
            "F1             : 0.2684\n",
            "ROC_AUC        : 0.7981\n",
            "\n",
            "üìä CONFUSION MATRIX:\n",
            "True Negative:  5686\n",
            "False Positive: 132\n",
            "False Negative: 1127\n",
            "True Positive:  231\n",
            "\n",
            "üîç –ê–Ω–∞–ª–∏–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏...\n",
            "\n",
            "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏:\n",
            "              feature  coefficient  abs_coefficient\n",
            "0             Recency    -1.216746         1.216746\n",
            "1           Frequency     0.538936         0.538936\n",
            "5      weekend_visits     0.043088         0.043088\n",
            "2            Monetary    -0.040363         0.040363\n",
            "4  total_unique_items    -0.019428         0.019428\n",
            "3           avg_check    -0.003765         0.003765\n",
            "\n",
            "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
            "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: logistic_regression_model_v1.pkl\n",
            "\n",
            "üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏...\n",
            "\n",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–∞—Ö:\n",
            "   threshold  accuracy  precision    recall        f1\n",
            "0        0.1  0.514214   0.269797  0.918262  0.417057\n",
            "1        0.2  0.683528   0.351060  0.792342  0.486548\n",
            "2        0.3  0.805741   0.486507  0.477909  0.482169\n",
            "3        0.4  0.828038   0.594801  0.286451  0.386680\n",
            "4        0.5  0.824554   0.636364  0.170103  0.268449\n",
            "5        0.6  0.820373   0.668293  0.100884  0.175304\n",
            "6        0.7  0.818841   0.730159  0.067747  0.123989\n",
            "7        0.8  0.816054   0.787879  0.038292  0.073034\n",
            "\n",
            "============================================================\n",
            "–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 2 –ó–ê–í–ï–†–®–ï–ù!\n",
            "============================================================\n",
            "\n",
            "üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n",
            "  –ú–æ–¥–µ–ª—å: LogisticRegression\n",
            "  –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 6\n",
            "  –¢–æ—á–Ω–æ—Å—Ç—å (accuracy): 0.8246\n",
            "  F1-–º–µ—Ä–∞: 0.2684\n",
            "  ROC-AUC: 0.7981\n",
            "  –°–∞–º—ã–π –≤–∞–∂–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫: Recency\n",
            "  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: -1.2167\n",
            "\n",
            "üîó –°—Å—ã–ª–∫–∞ –Ω–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç:\n",
            "  https://app.clear.ml/projects/307203333faf418db943144ad388677f/experiments/55111472edb24b66ade1b84ce77680c1\n",
            "‚úÖ –ó–∞–¥–∞—á–∞ –∑–∞–∫—Ä—ã—Ç–∞!\n"
          ]
        }
      ]
    }
  ]
}